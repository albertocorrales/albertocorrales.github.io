[ { "title": "Level Up Your AI Skills, Build a Custom MCP from Scratch", "url": "/posts/Build-your-own-MCP/", "categories": "", "tags": "AI, LLM, AI assistant, MCP, Claude, Typescript, pnpm", "date": "2025-07-13 12:00:00 +0200", "snippet": "IntroductionIn our previous article, we explored the Model Context Protocol (MCP) and learned how to use existing MCP servers to extend Claude’s capabilities. Today, we’ll take the next step and build our own custom MCP server from scratch. We’ll create a practical email validation server that can check if an email address is valid and detect potential spam addresses using a free API.This hands-on tutorial will walk you through the entire process of creating, testing, and deploying a custom MCP server using TypeScript and pnpm. By the end, you’ll have a solid foundation for building your own MCP integrations tailored to your specific needs.Project OverviewOur email validation MCP server will provide Claude with the ability to: Validate email address formats Check if an email is associated with spam or disposable email services Return detailed user information when available Integrate seamlessly with Claude’s conversation flowWe’ll use the UserCheck API as our validation service, but the principles apply to any public or private API you might want to integrate.The source code of the example is available in the Github repo: https://github.com/albertocorrales/mcp-example/Setting Up the ProjectLet’s start by creating a new TypeScript project with pnpm. This approach works with npm and yarn as well, but pnpm offers faster installation and better disk space efficiency.Project InitializationFirst, create a new directory for your project and initialize it:pnpm initInstalling DependenciesOur project needs two main dependencies:pnpm add @modelcontextprotocol/sdk zodLet’s break down what each dependency provides: @modelcontextprotocol/sdk: The official MCP SDK that provides all the tools we need to create MCP servers, including server classes, transport mechanisms, and type definitions. zod: A TypeScript-first schema validation library that helps us define and validate input parameters for our MCP tools. This ensures type safety and provides clear error messages for invalid inputs.Project ConfigurationUpdate your package.json to include the necessary configuration:{ \"name\": \"mcp-check-email\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"type\": \"module\", \"scripts\": { \"inspector\": \"npx -y @modelcontextprotocol/inspector npx -y tsx main.ts\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\", \"packageManager\": \"pnpm@10.12.1\", \"dependencies\": { \"@modelcontextprotocol/sdk\": \"^1.13.0\", \"zod\": \"^3.25.67\" }}The key configuration points: “type”: “module”: Enables ES6 module syntax in our Node.js project “packageManager”: Specifies the exact pnpm version for reproducible builds “inspector” script: Provides an easy way to test our MCP server using the official inspector toolUnderstanding the ImplementationNow let’s dive into the implementation. Create a main.ts file in your project root:import { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";import { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";import { z } from \"zod\";const server = new McpServer({ name: \"Check email MCP Server\", version: \"1.0.0\", description: \"Checks if an email address is valid and returns user information\"});server.tool(\"check-email\", \"Checks if an email address is valid and returns user information\", { email: z.string().email().describe(\"email address of the user to check\"), }, async ({ email }) =&gt; { const response = await fetch(`https://api.usercheck.com/email/${email}`); const data = await response.json(); console.log(data); return { content: [ { type: \"text\", text: `User information for ${email}: ${JSON.stringify(data)}` } ] } });const tranport = new StdioServerTransport();await server.connect(tranport);Let’s break down each part of this implementation:Server Initializationconst server = new McpServer({ name: \"Check email MCP Server\", version: \"1.0.0\", description: \"Checks if an email address is valid and returns user information\"});We create a new MCP server instance with metadata that helps identify and describe our server. This information is used by MCP clients to understand what our server does.Tool DefinitionThe core of our MCP server is the tool definition:server.tool(\"check-email\", \"Checks if an email address is valid and returns user information\", { email: z.string().email().describe(\"email address of the user to check\"), }, async ({ email }) =&gt; { // Tool implementation });This creates a tool with four key components: Tool name: “check-email” - This is how Claude will reference the tool Description: A human-readable explanation of what the tool does Schema: Zod schema that defines and validates input parameters Handler function: The async function that executes when the tool is calledInput Validation with ZodThe schema definition uses Zod to ensure type safety:{ email: z.string().email().describe(\"email address of the user to check\"),}This schema: Ensures the input is a string Validates that it’s a properly formatted email address Provides a description for documentation and error messages Automatically generates TypeScript types for the handler functionAPI IntegrationThe tool handler makes a request to the UserCheck API:const response = await fetch(`https://api.usercheck.com/email/${email}`);const data = await response.json();This demonstrates how to integrate with external APIs. The UserCheck API provides information about email validity, spam detection, and user details.Response FormattingMCP tools must return responses in a specific format:return { content: [ { type: \"text\", text: `User information for ${email}: ${JSON.stringify(data)}` } ]}The response contains an array of content blocks. Each block has a type and associated data. Text blocks are the most common, but MCP also supports images and other content types.The most powerful about the implementation of MCPs is the fact that we only have to return the data we get from the external API, and the AI assistant will interpret it, based on the structure and the name of the properties used in the response.Transport SetupFinally, we set up the communication transport:const tranport = new StdioServerTransport();await server.connect(tranport);StdioServerTransport enables communication through standard input/output, which is the most common way MCP servers communicate with clients.Testing with MCP InspectorBefore integrating with Claude, it’s crucial to test your MCP server. The MCP Inspector is a powerful tool that allows you to interact with your server directly.Running the InspectorUse the script we defined in package.json:pnpm inspectorThis command: Downloads the latest MCP inspector if not already available Runs your MCP server using tsx (TypeScript execution) Opens a web interface for testingUsing the Inspector InterfaceThe inspector provides a web-based interface where you can: View available tools: See all tools your server exposes with their descriptions and schemas Test tool calls: Execute tools with different parameters and see the results Debug issues: View detailed error messages and logs Validate schemas: Ensure your input validation works correctlyTesting Our Email CheckerIn the inspector interface: Select the “check-email” tool from the available tools list Enter a test email address in the email parameter field Click “Call Tool” to execute the request Review the response to ensure it contains the expected user informationTry testing with various email addresses: Valid emails: test@gmail.com Invalid formats: not-an-email Disposable emails: temp@10minutemail.comThe inspector will show you exactly what Claude would receive when calling your tool.Integrating with ClaudeOnce you’ve tested your MCP server with the inspector, you can integrate it with Claude.Configuration SetupAdd your MCP server to Claude’s configuration file. The location depends on your operating system: Windows: %APPDATA%\\Claude\\claude_desktop_config.json macOS: ~/Library/Application Support/Claude/claude_desktop_config.json Linux: ~/.config/Claude/claude_desktop_config.jsonAdd your server configuration:{ \"mcpServers\": { \"email-check\": { \"command\": \"npx\", \"args\": [ \"-y\", \"tsx\", \"D:\\\\dev\\\\mcp\\\\main.ts\" ] } }}Replace D:\\\\dev\\\\mcp\\\\main.ts with the actual path to your project’s main.ts file.Usage ExamplesAfter restarting Claude, you can start using your email validation tool in conversations. When you mention something about checking emails, the AI assistant will automatically detect that using the MCP for checking emails might be useful and it will ask for permissions to use it:This example is very basic, it can add new capabilities to our AI assistant that could make it really powerful if we use it properly. Here there are some usage examples where this new MCP would add new useful capabilities:Basic Email ValidationUser: Can you check if the email address john.doe@example.com is valid?Claude: I'll check that email address for you using the email validation tool.[Claude calls the check-email tool]Based on the validation results, john.doe@example.com appears to be a valid email format. The domain example.com exists and can receive emails, though this is a documentation domain typically used for examples.Bulk Email ValidationUser: I have a list of email addresses from a sign-up form. Can you check these for validity and potential spam?- user1@gmail.com- temp123@10minutemail.com - invalid-email@- admin@mycompany.comClaude: I'll check each of these email addresses for you:[Claude calls the check-email tool for each address]Here are the results:- user1@gmail.com: Valid email from a reputable provider- temp123@10minutemail.com: Valid format but this appears to be a temporary/disposable email service- invalid-email@: Invalid email format - missing domain- admin@mycompany.com: Valid format, domain exists and can receive emailsIntegration with Other TasksThe power of MCP really shines when combined with other capabilities:User: I'm reviewing user registrations from our database. Can you check the email addresses and create a report on data quality?Claude: I'll help you analyze the email data quality. First, let me check the email addresses in your database.[Claude uses filesystem MCP to read the database/CSV file, then calls check-email for each address]Based on the analysis of 150 email addresses:- 142 valid emails (94.7%)- 8 invalid format (5.3%)- 12 disposable email services (8.0%)- 3 potentially spam-related domains (2.0%)I recommend implementing email validation at registration time and consider blocking disposable email services for better data quality.Extending the Email CheckerThis basic example can be extended in many ways:Additional Validation Features Domain reputation checking: Verify if the domain has a good sending reputation Syntax validation: More sophisticated email format validation Mailbox verification: Check if the specific mailbox exists (with appropriate permissions) Bulk processing: Handle multiple emails in a single requestIntegration with Other Services CRM systems: Validate emails before adding to customer databases Marketing platforms: Clean email lists before campaigns User registration: Real-time validation during sign-up processesAdvanced Response Formats Structured data: Return validation results in a structured format Confidence scores: Provide confidence levels for validation results Recommendations: Suggest corrections for common email typosConclusionBuilding custom MCP servers opens up endless possibilities for extending Claude’s capabilities. The email validation example demonstrates the key concepts: Project setup with proper TypeScript configuration Tool definition with input validation using Zod API integration patterns for external services Testing workflows using the MCP Inspector Claude integration through configuration filesThe beauty of MCP lies in its simplicity and flexibility. With just a few lines of code, you can connect Claude to any API or service, creating powerful integrations adapted to your specific needs.Whether you’re building tools for data validation, API integration, database queries, or custom business logic, the patterns demonstrated here provide a solid foundation. The MCP ecosystem is rapidly growing, and by building your own servers, you’re contributing to a more connected and capable AI assistant experience.Start experimenting with your own MCP servers today - the possibilities are truly limitless!" }, { "title": "Getting started with MCPs, add super powers to your AI assistant", "url": "/posts/Getting-starting-with-MCPs/", "categories": "", "tags": "AI, LLM, AI assistant, MCP, Claude", "date": "2025-06-22 12:00:00 +0200", "snippet": "IntroductionAs AI assistants become increasingly integrated into our development workflows, the need for standardized ways to extend their capabilities has become paramount. Enter the Model Context Protocol (MCP) - a game-changing specification that’s revolutionizing how we connect AI models with external tools and data sources.The Model Context Protocol (MCP) is an open standard developed by Anthropic that enables AI assistants to securely connect with external data sources and tools. Think of MCP as a universal adapter that allows AI models to interact with your local files, databases, APIs, and custom tools in a standardized way.Why MCPs are important and how they workBefore MCP, extending AI capabilities required custom integrations for each use case. This led to fragmented solutions and limited interoperability. MCP solves these problems by providing: Standardization: A unified protocol that works across different AI models and tools, reducing development overhead and ensuring consistency. Security: Built-in permission controls and sandboxing mechanisms that allow you to grant specific access rights without compromising system security. Extensibility: A plugin-like architecture that enables developers to create reusable components for common tasks like file operations, database queries, and API integrations. Local Control: Unlike cloud-based extensions, MCPs run locally, giving you complete control over your data and tools while maintaining privacy. At its core, an MCP operates as a server that implements the protocol’s specification. When an AI assistant needs to perform an action, it sends a request to the MCP server describing what it wants to do. The MCP server then validates the request, checks permissions, and executes the requested operation using the appropriate tool or resource. For example, if the AI needs to read a file, it would call the appropriate file system tool provided by the MCP, which would handle the actual file operations in a secure, controlled manner. This architecture ensures that all interactions between the AI and external systems are mediated through the MCP, maintaining security and providing a consistent interface regardless of the underlying implementation.More information in the official MCP web.Examples of Using Existing MCPsThe MCP ecosystem already includes several powerful pre-built servers that demonstrate the protocol’s versatility. Let’s explore some practical examples:Filesystem MCPThe filesystem MCP is one of the most commonly used servers, allowing to interact with your local files and directories. Here’s what you can accomplish:Once configured, you can ask to: Read and analyze code files across your entire project Create new files with generated content Search for specific patterns across multiple files Reorganize directory structures Generate documentation based on existing codeExample interaction: “Please read all the TypeScript files in my /src directory and create a summary of the API endpoints defined in the route handlers.”SQLite and PostgreSQL MCPsThe SQLite MCP server enables you to query and manipulate SQLite and PostgreSQL databases directly:This allows for powerful database interactions like: Writing complex SQL queries based on natural language descriptions Analyzing data patterns and generating reports Creating and modifying database schemas Performing data migrations and transformationsExample interaction: “Examine my sales database and create a monthly revenue report for Q4, including a breakdown by product category.”Git MCPThe Git MCP server provides repository management capabilities:With Git MCP, you can: Analyze commit history and generate release notes Review code changes and suggest improvements Create branch summaries and merge reports Help with conflict resolution strategiesExample interaction: “review the commits from the last week and create a changelog for our next release.”GitHub MCPFor teams using GitHub, the GitHub MCP server enables direct repository interactions:This powerful integration allows the AI assistant to: Create and manage issues and pull requests Review code changes and provide feedback Search repositories and analyze project structures Generate automated reports on project activityExample interaction: “Create a pull request for the authentication feature branch and include a detailed description of the changes.”Brave Search MCPThe Brave Search MCP server gives access to real-time web search capabilities:With this server, you can: Get current information on any topic Research technical solutions and best practices Find relevant documentation and tutorials Stay updated on the latest technology trendsExample interaction: “Search for the latest best practices for implementing OAuth 2.0 with Node.js and summarize the key recommendations.”Getting Started with MCP Setup and ClaudeAs it was mentioned, MCPs can be used with all the popular AI assistants. In this section, we will configure Claude to use existing AI assistants. This example is based on Windows and NPX.As a prerequisite, we need NodeJS installed. Then, we need to go to the Claude desktop app. Open the menu File &gt; Settings &gt; Developer &gt; Edit Config. This will point you to the file claude_desktop_config.json, where you can configure your MCPs.Depending on the MCP you want to configure, the parameters might be different.For example, if we want to configure the MCP to work with the filesystem of our computer, we need to configure:{ \"mcpServers\": { \"filesystem\": { \"command\": \"npx\", \"args\": [ \"-y\", \"@modelcontextprotocol/server-filesystem\", \"C:\\\\Users\\\\username\\\\Desktop\", ] } }}With this configuration, we are allowing Claude to interact without filesystem in the Desktop folder.After this, we need to reset Claude. Then for example we can ask Claude to list the files in a particular folder:The first time Claude uses the MCP, it will ask for permissions. We can decide the scope of these permissions.ConclusionsThe Model Context Protocol represents a significant step forward in making AI assistants more useful and integrated into our daily workflows. By providing a standardized way to connect AI assistants with external tools and data sources, MCP opens up possibilities that were previously complex or impossible to achieve.What’s Next?This introduction has shown you the power of existing MCP servers and how to get started with basic configurations. The MCP ecosystem is still young, but it’s already clear that this protocol will fundamentally change how we interact with AI assistants. Starting with these existing servers and understanding the underlying concepts is the fist step to start mastering the evolution of AI assistants.Whether you’re looking to improve your development workflow, enhance your data analysis capabilities, or simply make Claude more useful for your daily tasks, MCP provides the foundation to make it happen. The examples we’ve explored today are just the beginning - the real power lies in combining multiple MCP servers and eventually creating your own custom integrations tailored to your specific needs.Start experimenting with these existing MCP servers today, and stay tuned for our next article where we’ll build a custom MCP server from scratch!" }, { "title": "Build Microservices at speed light with Copier", "url": "/posts/Copier-template-projects/", "categories": "", "tags": "Copier, Architecture, Tools", "date": "2025-01-26 11:00:00 +0100", "snippet": "Build Microservices at speed light with CopierIntroductionTemplating tools are critical in modern microservices architecture, solving key challenges in rapid and consistent software development. They address three primary pain points: Standardization: Ensure consistent project structure, configuration, and best practices across multiple services Productivity: Dramatically reduce setup time by automating repetitive initialization tasks Scalability: Enable quick replication of proven architectural patterns and configurationsIn microservices environments, where teams often build multiple interconnected services, templating tools eliminate manual setup, reduce human error, and maintain architectural coherence. They allow developers to focus on unique business logic rather than boilerplate infrastructure.Templating Tools ComparisonIn this section, we will analyze multiple alternatives to create, share and maintain templates that can help us to create templates that can be used as guidelines when we are creating new projects.YeomanYeoman is an advanced templating tool that allows you to create your own generators, so you can create your template projects.Its benefits include its many flexible automation use cases such as the ability to programmatically generate files in the template and even write unit tests on the generated files, which helps provide confidence in the templates and ensures developers have a productive and comfortable user experience.While its customizability and flexibility are benefits, they also can become downsides — more complex templates can become more confusing for developers. Questions of maintaining and testing Yeoman-based projects arise. Setting up your generators and plugins can be time-consuming, although once they’re in place, Yeoman is relatively easy to use for developers. In order to create generators, it requires developing in JavaScript. Then, your generators should be published to NPM, so others can consume them.CookieCutterCookieCutter is a templating engine that tries to simplify the way we create project templates. It is written in python, but it doesn’t require programming in python to create templates. You just need to create your project scaffolding and add your configuration in a file cookiecutter.json. With this file, cookie-cutter will ask for the prompt needed and it will replace the values in the corresponding parts following the jinja syntax.Additionally, you can create scripts that are executed after or before copying the generated files.CookieCutter doesn’t require releasing packages, you can just use mercurial URLs as arguments and it works based on that.CopierCopier is a newer project designed to bring the best of Yeoman and CookieCutter and get rid of the parts that are not so good.Similarly to CookieCutter, with Copier you don’t need to develop to create your templates. You just need to configure your scaffolding project and use the jinja syntax to manage replacements. It also works with a URL, so you don’t need to publish NPM packages.Comparing Copier with CookieCutter, the main advantage of Copier is the fact that it enables code lifecycle management for generated projects, including updating the answers provided in the prompts or syncing updates from evolved templates.The potential con, as it is a newer project, the community is not as big as its competitors. However, the support for the project has exponentially increased over the last few years.While tools like Yeoman and CookieCutter have been popular in the project templating space, Copier stands out with its unique approach and superior features: Extremely simple to use Robust update mechanism that allows template improvements to be applied to existing projects Language-agnostic design Minimal configuration overhead Clean and straightforward template definitionPractical Example: NodeJS Project TemplateHere’s a simple example of how you might create a Copier template for a NodeJS microservice:# copier.ymlproject_name: type: str help: Enter the name of your projectservice_port: type: int default: 3000 help: Enter the port number for the servicetemplate_path: .tasks: - git init - npm init -yAnd a corresponding template structure:{{ project_name }}/├── src/│ └── index.js├── package.json├── README.md└── .gitignoreTo generate a project from this template, you would simply run:copier copy https://github.com/yourusername/nodejs-microservice-templateFor any projects generated with copier, you run run the following command to get latest changes:copier updateWhen you run an update, copier will compare the version you have in your project and the latest version in the template. Then, it it will apply the diff to your project. If there are any git conflict, you will have the chance to resolve them manually, in a similar way you would do for a git merge.ConclusionsCopier represents a significant leap forward in project templating. Its simplicity, flexibility, and powerful update mechanism make it an excellent choice for developers looking to standardize and streamline their project creation process.Whether you’re working with microservices, maintaining multiple similar projects, or just want a more efficient way to bootstrap new development efforts, Copier offers a compelling solution that can save time and reduce repetitive setup work.As a developer who has extensively used Copier across several microservices, I can attest to its incredible utility. The tool has been instrumental in maintaining consistency, reducing boilerplate code, and ensuring that common configurations and patterns are seamlessly shared across different projects.The main features I love about Copier are: Simple and intuitive template creation Ability to update templates after initial project generation Supports multiple programming languages and frameworks Highly customizable template configuration Minimal dependencies and easy installation Versioning and conflict resolution based on Git" }, { "title": "CI/CD pipelines-as-code with Nuke", "url": "/posts/CI-CD-pipelines-as-code-with-Nuke-build/", "categories": "", "tags": "Azure DevOps, CI/CD, pipelines, Nuke", "date": "2023-06-05 12:00:00 +0200", "snippet": "IntroductionIn order to implement Continuous Integration and Continuous Delivery (CI/CD), we normally build pipelines. These pipelines are automatically triggered when we build new functionality, we push it into our source code repository, and it is merged into the master branch. As part of this process, we should have some automated or manual quality checks, such us code review, vulnerabilities scan, automatic testing, etc.Once the pipeline is triggered, it will guide the delivery process and how the changes are being promoted across multiple environments.Regarding Azure DevOps pipelines, it is a technology that has been evolving during the last few years as CI/CD has become more relevant. In the past, we used to have a build definition and a release definition, which were manually configured by dragging and dropping tasks. You can find more information about Azure DevOps pipelines in this link.More recently, Azure Pipelines introduced the YAML pipelines. This new way to define our pipelines was a very relevant improvement, because it allowed us to consistently reuse our pipeline definitions and define them dynamically. This is especially useful for complex pipelines, where we have multiple services or environments as part of our continuous delivery process.In order to build YAML pipelines, we need to use an specific syntax, which allows us to define our pipeline. This language includes concepts such as stages, jobs, tasks, variables, etc. In addition, we can use some expressions in order to dynamically build conditions, loops, built-in functions, etc.Limitations and disadvantages of Azure DevOps YAML pipelinesAzure DevOps YAML pipelines is an useful way to dynamically define our pipelines. However, in an enterprise context, where we might have very complex pipelines, YAML pipelines might have some limitations/disadvantages:1) YAML pipelines are tightly coupled to Azure DevOpsWhen we work with Azure DevOps YAML pipelines, we need to use their language. For that reason, if we want to run our pipeline in a different provider (like GitHub Actions), we won’t be able to do it.2) Working with YAML is tedious and error proneYAML is a human-readable data-serialization, which is commonly used for configuration files and in applications where the data is stored or transmitted. YAML has became very trendy, but it comes with some challenges, especially for complex YAML files or when we introduce programming features, such as the expressions in Azure DevOps. When we are configuring YAML, we might need to deal with issues such as indentation errors, typos or having huge files that are a mix of expressions, scripts, etc. As a result, maintaining those YAML files might be a real challenge. We can find this situation not only in Azure DevOps, but in other technologies like Terraform or Docker.3) We cannot test our pipelines locallyOne of the biggest limitations of Azure DevOps pipelines, is the fact that we cannot test or run our pipelines locally. In order words, when we are updating or creating a new pipeline, we need to edit our YAML file, push the pipeline definition to our repository, and run the pipeline in order to test it.During this process, we will often find out that there were some issues with our pipeline definition, so we need to repeat this process over and over. As a consequence, working with YAML pipelines might slow us down.4) YAML size limit of 2MBsWhen we are building YAML pipelines, we normally use templates, so we can reuse the same set of tasks, jobs or stages across our pipeline. Using templates is really helpful to make our templates simpler and more maintainable. However, when Azure DevOps triggers a build, it will generate an extended YAML file for the pipeline.As an example, if we have a template to deploy a lambda function with length 30 lines of YAML and we use that template to deploy 10 lambdas in our pipeline, it will generate an extended YAML file with 300 lines. This situation can get worse and worse when we are working with complex pipelines, where our CI/CD process includes many services or many environments. Working with the extended YAML file can be a serious limitation for complex pipelines, as Azure DevOps imposes a size limit of 2MB for the extended YAML file.In order to try to mitigate this limitation, we can try to work with the strategy matrix. When we use the strategy matrix, the extended YAML won’t duplicate the YAML. However, it only works at job level and it might be not very effective if our pipeline is using different templates for building/deploying different type of projects.Using Nuke BuildWhen it comes to tools for replacing YAML in Azure DevOps pipelines, there are multiple projects which follow different approaches, but with the same goal. Some examples are: Cake Build Bullseye Sharpliner Nuke BuildMost of these tools use C# as programming language, as it is the most popular language for developers who work with Microsoft technologies. This is a great advantage, as C# is a rich strongly typed language and there are libraries to do anything you can need your pipelines.Apart from that, these tools provide an SDK that will make easier building pipelines.From the tools mentioned above, Nuke Build is the most popular and advanced one. Some of the features that make Nuke Build a great choice are: It is very easy to install and start working with it. It is agnostic, so you can run it on any providers. It provides a rich developing experience and the SDK includes common tasks you might need in your pipeline. You can run and debug your pipelines locally. It provides a CLI which is really helpful to set new projects, run a plan, etc. It provides support for the most popular CI/CD vendors.How to setup a new Nuke projectSetting up a new Nuke project is quite straightforward. First of all, you have to install the global tool with the following command:dotnet tool install Nuke.GlobalTool --globalThen, you can setup your nuke project with the command:nuke :setupThis is how it looks the example build created by Nukeusing Nuke.Common;using System;class Build : NukeBuild{ public static int Main () =&gt; Execute&lt;Build&gt;(x =&gt; x.Compile); [Parameter(\"Configuration to build - Default is 'Debug' (local) or 'Release' (server)\")] readonly Configuration Configuration = IsLocalBuild ? Configuration.Debug : Configuration.Release; Target Clean =&gt; _ =&gt; _ .Before(Restore) .Executes(() =&gt; { Console.WriteLine(\"Clean\"); }); Target Restore =&gt; _ =&gt; _ .Executes(() =&gt; { Console.WriteLine(\"Restore\"); }); Target Compile =&gt; _ =&gt; _ .DependsOn(Restore) .Executes(() =&gt; { Console.WriteLine(\"Compile\"); });}Now we can run the build just with the command nuke or from VisualStudio, we can open the project and run it as any other dotnet program. Apart from that, we can easily set breaking points and debug our nuke builds in a very convenient way.Apart from this, we can check the plan of our build. This can be very useful for complex builds. In order to do so, we can run the command:nuke --planAnd we will display the flow diagram of our build:Integration between Nuke and Azure DevOpsIf you are working with a very simple pipeline, Nuke might be all you need to build it. However, for more complex pipelines, you might need a pipeline with multiple jobs or stages running in parallel. For these scenarios, you can still work with Nuke to build the tasks of a job, but combine it with the concepts provided by Azure DevOps such as stages, jobs, etc. This will allow you to run different jobs/stages in parallel (this is a limitation for nuke at the moment). In addition, you can leverage the permissions model for Azure DevOps, which can be handy if you need approvals for promoting your changes to production environments.By following this approach, we would have a small part of YAML that defines the structure of your pipeline with the stages/jobs, but most of the pipeline logic would be done by Nuke projects.In this case, for each job, our YAML template would look similar to this:parameters: - name: Paramter1 type: string - name: Parameter2 type: stringjobs: - job: myJob steps: - bash: ./build.sh displayName: CI workingDirectory: \"$(Build.SourcesDirectory)/Build/MyProjectExample\" env: Paramter1: ${ { parameters.Paramter1 } } Parameter2: ${ { parameters.Parameter2 } }On the other hand, if we want to get rid of the whole YAML, we could use Sharpliner to implement the definition of stages and jobs and combine it with Nuke. Sharpliner generates Azure DevOps pipelines in YAML from C#. For that reason, it can be helpful to reduce the errors writing YAML, but we could still hit the size limit for YAML if we overuse it.Nuke build best practicesIn this section we will review some good practices and recommendations based on my experience working with Nuke:1) Reuse targets:In order to prevent code duplication, we should always build our targets as generic targets and reuse them whenever it is possible. You can always use parameters to customize certain options in your target.2) Use dependencies properly:The dependencies model in Nuke can be a little complex, especially when you have multiple combinations of dependencies between your targets. As a general rule, using TryAfter and TryBefore should be the way to define our dependencies, so we don’t trigger Targets that we are not scheduled in the pipeline project.DependentFor and DependOn should be used only if there is a strict dependency and that dependency will never change or be optional. You have more info about Nuke dependencies in this link.3) Idempotent builds:Our pipelines should be idempotent, so no matter how many times we run them, they should produce the same outcome. In order to achieve this, we can easily clean up temporary files that we create during the build process with the method EnsureCleanDirectory provided by Nuke.4) Single-responsibility in targets:When we are building targets, these targets should follow the Single-responsibility principle, so we don’t have targets that are doing more than one thing. On the other hand, we should also prevent building targets that are too small and they don’t provide any value by themselves.ConclusionsAs we have seen in this article, working with YAML to build our CI/CD pipelines can bring some challenges and limitations. In order to deal with these issues, Nuke allows us to build our pipelines as code, so our pipelines will be less error prone, more maintainable and easier to develop/debug.Using Nuke has a learning curve, but compared to other frameworks it is the most advanced one at the moment and it brings a very good developing experience, especially if you are already using C# in your projects.However, depending on the complexity of your build, you might need to find a balance and use a mix of Nuke and the CI/CD engine definition. By following this approach, you would have Nuke to cover most of the logic in our pipelines, but you could also leverage features such us jobs/stages parallelization, permissions management, etc." }, { "title": "Working with Event Sourcing, CQRS and Web Sockets on AWS", "url": "/posts/Working-with-event-sourcing-and-web-sockets/", "categories": "", "tags": "aws, EventStoreDB, event sourcing, CQRS, WebSockets, Architecture", "date": "2023-01-18 11:00:00 +0100", "snippet": "IntroductionThe WebSocket API is an advanced technology that makes it possible to open a two-way interactive communication session between the user’s browser and a server. With this API, you can send messages to a server and receive event-driven responses without having to poll the server for a reply.We normally use WebSockets when we want to get live updates of a particular object without having to constantly poll for new updates. In this scenario, WebSockets can be particularly helpful, as having many clients polling our APIs might increase the cost of our infrastructure, and if the number of request is very high, the API might apply throttling.In this article, we will review how we could use WebSockets for EverntSourcing+CQRS. Nevertheless, you can follow the same approach for architectures that are not using that pattern, but you need to notify the clients when there are changes in a certain dataset.ArchitectureWhen we are using EventSourcing and CQRS, we have commands that issue events. These events are projected into the state database. Then, we have queries that allow us to efficiently query the read models that we store in the state database.In some scenarios, we might want to be notified in the frontend every time there is a change in a particular object, so we can refresh the UI with the latest data. Examples of these are chats, dashboards, push notifications, etc.When we are using Event Sourcing and CQRS, we can leverage the capabilities of the projections for notifying our clients through WebSockets when the data changes.In this architecture, from the client perspective, it only requires creating a new WebSocket connection and reacting when there are new events coming through the socket.If we are using DynamoDB as our state database, an alternative to this could be using Streams on DynamoDB. In this case, when te data changes, DynamoDB streams invoke a lambda, which will notify the corresponding sockets. The disadvantage of this approach is the fact that it won’t work with other data stores (ElasticSearch, Neptune, etc). For this reason, notifying during after projecting the data is more generic.Regarding the data we send though the WebSocket, ideally we would be sending the model we want to render in the frontend, so we can use directly the models we get through the WebSocket. However, this might not be suitable for some scenarios, as we night need to apply permissions or data aggregation for the DTOs we serve in our queries. If this is the case, we can just send a notification to our clients telling them that the data has changed, so they fetch latest from the Query API.SecurityFor security reasons, the data we send though the socket must be encrypted. For this, it is important using secure WebSocket connections (wss).Additionally, if our data is not public, we should implement authentication, so we ensure that only authenticated users can receive updates from the WebSocket.The authentication on WebSockets works in a similar way as for HTTP APIs, so when we request creating a new connection, we send the access token. Then, the token will be validated by the authorizer in the API Gateway. However, how we pass the token in a WebSocket connection request might be a little different. For http request, we normally use the header authorization, but we cannot use custom headers in WebSockets when we are working with javascript. For this case, there are several alternatives, such as sending the token in the socket protocol or as a query parameter.Apart from that, there are other alternatives that allow you to create the connection without authentication, and then the client sends the token in the first message, once the connection is open. The problem with this approach is the fact that we would be allowing the clients to create connections without passing the authentication, which might be less secure. For this reason, I would advise validating the token before the client creates a new connection.WebSockets on AWSIn order to implement WebSockets on AWS, API Gateway provides built-in support, so the service will automatically manage the communication between clients and server. In order to implement WebSockets, we need to create at least two lambdas: one for creating new connections and another lambda for closing connections. Optionally, we can create other lambdas for sending data.Once you get a request for creating a new connection, you need to store the connection in a data store. In a similar way, when a connection is closed, you need to remove it from the store. This store will be used to know which connections we need to notify when there is a new update. If we try to send data to a connection and the connection is GONE, we can delete the connection from the data store. In our architecture, we store the WebSocket connections in MemoryDB for performance reasons, but you could pick any data stores you prefer.In spite of the fact that you can you can use the default domain that AWS API Gateway generates automatically, it is recommended creating your own subdomain (for example wss://websockets.my-domain.com) and create a new endpoint in API Gateway for WebSockets.Here you have examples of WebSockets applications in javascript and csharp.WebSockets JavaScript clientsIn order to implement your javascript client, you can use the class WebSocket.If you are using react hooks, our code would look similar to this example:const [socket, setSocket] = (useState &lt; WebSocket) | (undefined &gt; undefined);useEffect(() =&gt; { if (!socket) { const webSocket = new WebSocket( \"wss://websockets.my-domain.com/notifications\", accessToken ); webSocket.addEventListener(\"open\", async () =&gt; { await refreshMyData(); }); webSocket.addEventListener(\"message\", async (event) =&gt; { if (event.data === \"DataUpdated\") { await refreshMyData(); } }); setSocket(webSocket); } const fiveMinutesInMilliseconds = 300_000; const interval = setInterval(() =&gt; { if (socket &amp;&amp; socket.readyState === socket.OPEN) { socket.send(\"PING\"); } }, fiveMinutesInMilliseconds); return () =&gt; { if (interval) { clearInterval(interval); } if (socket) { socket.close(); } };}, []);In this example, we would be fetching the latest data with the method refreshMyData() every time there is an update. In addition, once the connection is open, we fetch the data, so we are sure that we are displaying the latest version of the object. This part wouldn’t be needed if you only care about new events (for example in a chat room).In addition, you should also bear in mind the AWS API Gateway quotas. For example, AWS API Gateway would close any connections that have been idle for more than 10 minutes. If you don’t want this to happen, you should ping the WebSocket from time to time in order to keep it alive. In the example above, we just create an interval that pings the socket every 5 minutes.Finally, when you unload the component in the UI, you should clear ping interval (if you are using any) and close the connection.Testing Web SocketsIn order to mock the socket in unit tests, you can use the npm package jest-websocket-mock, so you can mock it and check that is it is working as expected.For integration tests, you can implement integration tests where you subscribe to a particular object with a web socket. After that, you can issue commands and check if you are getting the updates through the web socket.Finally, for end-to-end tests, we don’t need to do anything specific. However, if you are using Cypress, I spotted an issue with Cypress when we are working with WebSockets, as it closes the before receiving the handshake response.If you are facing this issue, you need to bypass the WebSockets endpoint in your cypress configuration, so cypress won’t proxy these requests. For that, we would add this custom configuration in the file cypress.config.ts.import { defineConfig } from \"cypress\";export default defineConfig({ setupNodeEvents(on, config) { on(\"before:browser:launch\", (browser = {}, launchOptions) =&gt; { launchOptions.args = launchOptions.args.map((arg) =&gt; { if (arg.startsWith(\"--proxy-bypass-list\")) { return \"--proxy-bypass-list=&lt;-loopback&gt;,wss://websockets.my-domain.com\"; } return arg; }); return launchOptions; }); },});ConclusionsIn this post, we have seen what WebSockets are and how we can use them to reduce the number of requests we perform to our APIs when we need to keep updated our clients.As we have discussed, WebSockets can be specially relevant to prevent polling our APIs, which could overload them, increasing the cost or even consuming part of the traffic quota.In this post, we have also analyzed an architecture that leverages Event Sourcing and CQRS to send updates through WebSockets, so the web clients can get updates in real time.Finally, we have explained how WebSockets can be implemented, including the backend with AWS API Gateway + Lambdas, and the frontend with the provided API for WebSockets." }, { "title": "Scaling up/down EventStoreDB without downtime", "url": "/posts/Scaling-up-down-event-store-without-downtime/", "categories": "", "tags": "aws, EventStoreDB, event sourcing, CQRS, cluster, scaling", "date": "2022-10-14 12:00:00 +0200", "snippet": "IntroductionEventStoreDB (ESDB) is a log stream database oriented to architectures based on event-sourcing. The data in EventStoreDB is stored into streams, and each stream contains a log of events. For this reason, ESDB is a very suitable candidate for working with event-sourcing architectures and it allows us to easily implement the CQRS pattern, where there are projections that transform the events produced by the commands into data that can be consumed by the queries.In EventStoreDB, there are three types of nodes: Leader, Follower and ReadOnly. The Leader node is the node where we write or read data and there can be only one Leader in the cluster. Follower nodes replicate the changes from the Leader node and they are not directly used for writing or reading data. However, if the Leader node is down, one Follower can be re-elected as the new Leader. For high-availability, it is recommended to have at least three nodes in three different availability zones (one leader and two followers). Finally, ReadOnly nodes are a special type of node that we can only use for reading data. This node will replicate the changes from the Leader. ReadOnly nodes cannot be Leaders or Followers, so they won’t participate in re-elections.When we are using event-sourcing, we will normally be managing many projections, so if we are using one or more ReadOnly nodes, our projections read from them and would prevent overloading the reader node. However, there are limitations and adding ReadOnly nodes dynamically (scaling-out) might not be a good idea for multiple reasons: As we add more nodes, they will be replicating the data from the Leader node, so that will put more pressure on the leader node. Even if we add more nodes to a cluster, the existing projections won’t use them, as they already have their connections established with a particular node.For this reason, when it comes to provisioning EventStoreDB, planning in advance the demand we are going to need and scale up/down the cluster is a more suitable approach.In this article, we are going to see how we can scale up/down EventStoreDB without downtime leveraging the cluster capabilities. I’m going to focus on scaling up/down, but this approach can also be used for applying changes that require downtime, such as upgrading the SO, the version of ESDB, etc.Scaling up/down EventStoreDBIn order to run EventStoreDB, we are using AWS EC2. In EC2, if we want to change the type of instance, we have to stop the instance, change the instance type and start the instance. In addition, once the instance has stated, the node will have to catch up. We can keep checking the endpoint /health/live, and when it returns 204, the node has caught up.It is not a complex process, but if we apply it for a running instance, it will require downtime. However, if we are using a cluster, we can leverage this to scale up/down the cluster without downtime.In order to do so, we will update first the ReadOnly nodes, as they can be removed from the cluster and the consumers will automatically start reading from the Leader node. Then we will update the instance type for the Follower nodes. Since nobody is reading/writing in this nodes, we can turning them off without affecting any consumers, but it is recommended to do it sequentically, so we allways have at least two nodes running. Finally, we will update the Master node. However, after doing this, we will reduce the priority of this node and force a re-election, so this node will be a Follower when we turn it off. As a result, it won’t affect any producers or consumers.This process is described in the following diagram, where we have a cluster with four nodes, 1 Leader, 2 Followers and 1 ReadOnly, and we want to update the instance time from T1 to T2:Architecture and implementationFor the implementation, we have a lambda which implements a finite-state machine. The current status of the update is stored in a JSON object. This object is sent to the lambda as input. On the other hand, the lambda returns the same object as output, with the corresponding updates. In this way, we can track the status of the update and how it is making progress across every iteration. The lambda will be invoked by a step function every 5 seconds. Then, the step function will manage the long-running process, so the lambda execution time is short. This is more cost effective and prevents timeouts in the lambda, as updating all the nodes of the cluster can take some time.The step function will also tack the outcome of the process, so it can finish the step function as success or failure.This is the graph view for the step function:For the initial input of the step function, we just need to configure the type of update (as this approach can be used for other type of updates) and the instance type:{ \"UpdateStrategy\": \"Ec2InstanceTypeUpdate\", \"TargetInstanceType\": \"t2.xlarge\"}Then, for every iteraction, we will have an output what will show us how the process is making progress. For example, in the following output, we see that nodes test-node4readonly and test-node3 were already successfully updated, node test-node is in progress and the node test-node2 is pending. We can also see the number of iterations for each node. In addition, we have a parameter MaxNumberOfIterationsPerNodeUpdate that will allow us to timeout the update if any node updates exceed that threshold.{ \"EventStoreNodeUpdates\": [ { \"NodeName\": \"test-node4readonly\", \"NodeUpdateStatusDetail\": \"Success\", \"NodeUpdateStatus\": \"Success\", \"NodeType\": \"ReadOnlyReplica\", \"NodeAddress\": \"test-node4readonly.example.com\", \"NodePort\": 2113, \"Ec2InstanceId\": \"i-04d277e342129fe6c\", \"OriginalNodePriority\": 3, \"CurrentNodePriority\": 3, \"UpdateIterations\": 19 }, { \"NodeName\": \"test-node3\", \"NodeUpdateStatusDetail\": \"Success\", \"NodeUpdateStatus\": \"Success\", \"NodeType\": \"Follower\", \"NodeAddress\": \"test-node3.example.com\", \"NodePort\": 2113, \"Ec2InstanceId\": \"i-03191b6f020b34580\", \"OriginalNodePriority\": 2, \"CurrentNodePriority\": 2, \"UpdateIterations\": 18 }, { \"NodeName\": \"test-node\", \"NodeUpdateStatusDetail\": \"CaughtUp\", \"NodeUpdateStatus\": \"InProgress\", \"NodeType\": \"Follower\", \"NodeAddress\": \"test-node.example.com\", \"NodePort\": 2113, \"Ec2InstanceId\": \"i-017bbfe8345fcf55e\", \"OriginalNodePriority\": 0, \"CurrentNodePriority\": 0, \"UpdateIterations\": 5 }, { \"NodeName\": \"test-node2\", \"NodeUpdateStatusDetail\": \"Pending\", \"NodeUpdateStatus\": \"Pending\", \"NodeType\": \"Leader\", \"NodeAddress\": \"test-node2.example.com\", \"NodePort\": 2113, \"Ec2InstanceId\": \"i-061d6f4c1fj38f17\", \"OriginalNodePriority\": 4, \"CurrentNodePriority\": 4, \"UpdateIterations\": 0 } ], \"ClusterUpdateStatus\": \"InProgress\", \"UpdateStrategy\": \"Ec2InstanceTypeUpdate\", \"TargetInstanceType\": \"t2.xlarge\", \"MaxNumberOfIterationsPerNodeUpdate\": 1000}We could also implement the finite-state machine as part of the step function, but having the logic in the lambda gives us some benefits: It is easy to maintain, test and extend. It simplifies the approach. To implement that process with one lambda per step, we would’ve needed more than 10 lambda functions. It is not coupled to step-functions. We could run the same process from any source, for example as an SSM automation. It gives us flexibility, since we can customize the current state and the lambda will continue the process. This can be handy if something goes wrong in the middle of the process and we want to resume it from the last valid state.Workflow and extensibilityIn order to perform the update of the cluster, we have implemented an strategy pattern, so we can implement multiple types of updates for EventStore cluster. Then, we have two main classes: the class ClusterUpdateService that orchestrates the update across the nodes of the cluster, and the class UpdateInstanceTypeStrategy that implements how we update the type of an EC2 instance.If we observe the workflow, we can see that the process starts by ordering the nodes of the cluster, so we first process the read-only nodes, then the follower and finally the leader node. In order to get the information from the nodes, we will use the /gossip endpoint, and we will also check this endpoint for every iteration, in case the role of any node had changed.To decide which is the current processing node we select the first node with NodeUpdateStatus equals to InProgress or Pending.If the current node is a Leader node, we need to reduce its priority and force a re-election until the node has the role of Follower. You can find more information about the leader resignation process in this link.Once our node it is ready to be updated, we stop the EC2 instance, change the instance type and start the EC2 instance again.After doing that, we have to check if the node caught up, so we can start processing the following node or finish the process if there are no more pending nodes.If there were any failures during the update, the process will stop and the step function will finish as failed.ConclusionsIn this article we have seen how an EventStoreDB cluster works and how we can leverage this to perform updates to the cluster progressively, so we prevent having downtime.It has also be proposed an implementation that uses AWS serverless services, providing a solution based on AWS lambda and AWS step functions. For now solution is being used to update the EC2 instance type, but it can be easily extended for other types of updates, as it implements an strategy pattern.This process can be triggered manually based on some metrics that we are actively monitoring or we could also configure AWS CloudWatch alarms that trigger the step function automatically via EventBridge." }, { "title": "Dropping the cost of DynamoDB", "url": "/posts/Dropping-the-cost-of-dynamodb/", "categories": "", "tags": "aws, dynamodb, cost saving, cloud, serverless", "date": "2022-08-28 12:00:00 +0200", "snippet": "IntroductionDynamoDB is the most popular NoSql database on AWS. It allows you to easily build high performant applications taking care of the good practices, such as: encryption, access control, high-availability, backups, etc.DynamoDB allows you to configure two types of capacity allocation: provisioned and on-demand, and it defines the capacity with read capacity units (RCU) and write capacity units (WCU). One RCU is one strongly consistent read or two eventually consistent reads for a data block of up to 4 KB. On the other hand, one WCU represents write request for a data block of up to 1 KB.When it comes to pricing, it is important to understand these two modes and pick the one that fits better for each case, so we can have tables that auto-scale without over-provisioning.In this post, we will see a real case, where picking the right mode helped to drastically reduce the cost of using DynamoDB.Provisioned modeWith provisioned capacity, you can configure the maximum RCU ans WCU of the table and this usage will be charged hourly. In addition, you have to configure RCU and WCU for the indexes of the table.When you use provisioned mode, if the application exceeds the defined limits, the exceeding requests will be throttled. From the user perspective, it could be reflected as write or read requests being temporary denied, which is not a good user experience.Normally the traffic of a table tends to be variable, for example high during working hours and low during non-working hours. Or we might have spikes of traffic for more demanding processes like a data migration.When you use provisioned mode and the traffic of the table is variable, you might have to configure a high limit for RCUs and WCUs, so you prevent rejecting requests. The problem with this approach, is the fact that you would be over-provisioning and paying for a high capacity that is not being used. In addition, planning the capacity needed for each table and index of the table might be challenging, because it can change over time.In order to try to make the provisioned mode less rigid, DynamoDB allows you to configure auto-scaling policies. These policies allow you to configure values for min ans max capacity and the percentage of target utilization that will be trigger the alarm to scale up.However, there are two important limitations with this approach: You still need to have a minimum capacity and pay for it, even if you are not using it. When DynamoDB scales up, it reacts very slowly, so you will end up having throttling for RCUs and WCUs. In this article we can see good examples of this, where the dynamodb is scaling up too late, so there will cause throttling.On-Demand modeWith on-demand capacity, pricing is based on the amount of read and write request units the application consumes throughout the month. With on-demand mode there are not up-front charges for the DynamoDB capacity and it will auto-scale following these rules.The most interesting part of on-demand is the fact that when you create a new table with on-demand mode, you will serve up to 4,000 write request units and 12,000 read request units, but you don’t have to pay provisioning that capacity and your workload won’t be throttled for using this capacity. For this reason, on-demand is the best mode to work with frequently unpredictable workloads and it is what AWS recommends for these scenarios.Taking the best of provisioned and on-demandOn-demand mode in dynamodb can help us to reduce the cost for our tables, so we only pay for the requests we perform, instead of over-provisioning by using fixed RCUs or WCUs. However, when you are using on-demand mode, you cannot set any limits on WCUs or RCUs to prevent unfair usage of the table. This can be a risk, because if a table is having a constant very high load, it might lead to a huge increment in the cost.Being able to establish limits is one of the good features about provisioned mode. For example, imagine we want to limit a table with 100 RCUs and 150 RCUs, but we don’t want to pay for that capacity if we are not using it. In addition we don’t want to have throttling when we go from 0 RCUs to 100 RCUs. In this scenario is when we can use a hybrid model, where we normally work with on-demand mode, but we establish limits for RCUs and WCUs, so if we exceed one of those limits, we change the mode of the table to provisioned to start applying throttling. This will restrict the traffic for the affected dynamodb table, so can prevent unfair usage of it.The proposed architecture is represented by the following diagram:In the diagram, we can see that for each of our tables that we configure as on-demand, we define AWS CloudWatch alarms. When any alarms are triggered, AWS sends an event to the default bus of AWS EventBridge, so we can subscribe to those alarms and react to them. In order to be able to filter out these alerts, we defined a concrete naming convention, so in the EventBridge target, we only subscribe to alarms which starts with dynamodb-threshold-alarm#:dynamodb-threshold-alarm#{TableName}#{AlertName}In addition, from the alarm name, we know the name of the table that triggered the alarm and the alarm type (for example table-rcu, index-wcu, etc).When we receive any of these alarms, we trigger a step function, which will switch the mode of the table from on-demand to provisioned. In order to establish the limits for each table, we will store those values in a DynamoDB table.In the step function, we will setup the needed logic to perform the mode update. When we change from on-demand to provisioned, we will have to keep the provisioned mode for 24 hours, since DynamoDB only allows one switch every 24 hours. After that, we can switch back to on-demand. We will also have to check if for a given table we already have an update in-progress. In that case, we will skip the flow.Bear in mind that updating from on-demand to provisioned shouldn’t happen too often. If that is the case, we might be having unfair usage of the table, or our the limits we establish were too restrictive, so we should review it. In order to notify us, we can send the same alerts to our email or any provider we are using to manage incidents, like PagerDuty.Cost reduction resultsWhen we starting using DynamoDB, we used provisioned mode as the default mode, since there was a requirement for setting up usage limits in our DynamoDB tables. We work with a big number of DynamoDB tables, since we have independent tables for each micro-service. In addition, for each tenant the data is stored in a different table for segregation and security purposes. In order to prevent throttling, we have to set high thresholds for RCUs and WCUs, especially in those tables with more traffic.As you can see in the plot, as we are adding new clients and micro-services, the cost of having DynamoDB started growing more and more between April and May.In June, we starting progressively applying the approach presented in this article to some of the tables. As a result, we started to see how the cost was decreasing.In July and August, we already had applied the new approach for most of our DynamoDB tables and the cost went flat, in spite of the fact that we continue adding more DynamoDB tables due to new clients onboarding or new micro-services being setup.ConclusionsIn this article, we have seen the different modes that DynamoDB offers, and how they can impact our budget. Then, it has been presented the approach we followed to reduce the cost of DynamoDB, by leveraging the best of provisioned and on-demand modes.In the cost analysis, we have seen that now we are effectively managing the cost of DynamoDB, which will allow us to keep growing as a platform.In the future, it would be a great improvement if AWS implements a built-in feature to establish limits for on-demand mode, so we don’t have to switch to provisioned mode. Because it makes sense to use a pay-as-you-go model, but being able to setup limits. I’ve sent this feedback to AWS, so I hope they consider adding something like that.Finally, bear in mind that this approach might not fit for every case, so it is important to understand how your data stores are being used and how DynamoDB pricing works, so you can apply the best strategy for your case." }, { "title": "Run docker on windows easily without docker desktop", "url": "/posts/Run-docker-on-windows-without-docker-desktop/", "categories": "", "tags": "docker, rancher, tools, development, wsl", "date": "2022-07-22 12:00:00 +0200", "snippet": "IntroductionDuring the last years few, Docker has become one of the most popular technologies for vitualizing all kind of services. Until last year, if you were working with Docker, you were probably using the Docker Desktop application, which nicely installs and manages your docker containers, images, kubernetes, etc.However, at the beginning of this year, Docker changed their policy and now you can use docker desktop without paying a license only for personal usage. But enterprise usage requires a license.The positive side is that you can still use for free docker-cli, docker engine, etc, so you can still run docker containers.If you are working on Windows, you are going to be able to run docker on WLS 2. WSL (Windows Subsystem for Linux) allows you to run a Linux distribution on Windows. In order to configure it, there are many articles that explain how to do it (for example this and this. This process can very tedious, specially when you find that some commands are not working for you. But you can manage to configure it with some researching.When you configure docker on Ubuntu, you will able to run docker commands or even install docker-compose in Linux. But the integration with Windows can be not as good as with Docker Desktop. This can make a little harder to work with tools like FluentDocker.However, I recently tried Rancher Desktop, and it simplifies a lot working with docker on Windows, offering a similar experience as working with Docker Desktop.Working with Rancher DesktopRancher Desktop for windows is a very straightforward application. You just install it as any other applications for Windows, selecting dockerd as container runtime.Under the hood, rancher is managing for you all the complexity of creating a Linux subsystem and configure it to work with docker. In addition, it offers a great experience when you are working with Windows, since all your docker commands will work automatically, including docker-compose or kubernetes.The only issue that I had when I installed rancher, is the fact that I couldn’t pull any docker images.This is happening because the DNS is not properly configured. I also had that issue when I tried to configure docker on Ubuntu WSL2.If you are having the same issue, open the command console, go to the linux system created by rancher with the command:wsl -d rancher-desktopThen go to the file /etc/wsl.conf and update the value generateResolvConf to false.Finally go to the file /etc/resolv.conf and add replace the content with:options use-vcnameserver 8.8.8.8Now you can check that docker commands are pulling the images properly:When you run an image with docker rancher, it will run in localhost, so applications like FluentDocker will continue working as always. Apart from that, it also works with docker-compose and kubernetes.One feature that I like is hte fact that you can list and manage your images:But there you can not see or manage your containers. For that I would recommend using this VS Code extension, which allow you manage images and containers:ConclusionsIn this post, we have seen different alternatives to run Docker on Windows without Docker desktop, since it is not a free tool anymore for companies.In particular, we have seen how to install and configure Docker Rancher, which gives us a very easy way to keep working with docker on our Windows machines." }, { "title": "Migrating your blog from Blogspot to Github pages", "url": "/posts/Migrate-blogspot-to-github-pages/", "categories": "", "tags": "markdown, blog, tutorials, github, jekyll", "date": "2022-07-10 12:00:00 +0200", "snippet": "IntroductionI’ve recently migrated my blog from Blogspot to Github Pages. The main reasons to do are: I wanted to use markdown as language to edit my posts. Markdown is becoming the standard to write documentation, so I would like to have my posts in a format that I can easily migrate or post in other platforms. Blogspot is a great tool for editing text, but it is not so good for adding source code. In spite of the fact that you can edit your template and add scripts, the highlighters I’ve tried were not very good. However, writing source code in markdown works pretty well and it supports many languages.Then, in order to migrate the content of my blog, I tried multiple ways and I wanted to document the steps that worked better for me, in case it can help other people.Steps to migrate your blogCreate your blog at Github pagesFirst of all, you will need a blog in Github pages. In order to create your blog template, you can use Jekyll, which is an open source project to convert your plain text into static websites or blogs.For the theme of your blog, you will have many options, free or paid. In this website you can find many examples: https://jekyllthemes.io/For this post, I’ve chosen Beautiful Jekyll, because it simple and it has a free plan, which can be enough for your needs.In order to setup your blog with beautiful Jekyll, you just have to fork this repository [In order to setup your blog with beautiful Jekyll, you just have to fork this repository https://github.com/daattali/beautiful-jekyll, rename the project to be like [https://.github.io](https://.github.io), and configure the file `_config.yml`.Then, you just need to add your posts as markdown files in the folder _posts, and they will be automatically published in your blog. You will be able to see your blog at [https://.github.io](https://.github.io).Apart from that, you can also run your blog based on Jekyll locally. In order to do so, you will have to install Ruby, RubyGems, GCC and make. You will find more instructions for your operating system here: https://jekyllrb.com/docs/installation/.Once you followed the installation guide, you will be able to run the following command and your blog will be available at http://localhost:4000.bundle exec jekyll serveMigrate your posts1) Import your posts from BlogspotOnce, you have your Github blog setup, you will need to import your posts from blogspot. For this step, I’ve tried different methods and this is the one that worked better for me.To import your content at Blogspot, you will need ruby installed. Then, you have to run the script that I shared in this Github gist.In order to run the script you can copy it to the root folder of your blog repository, and then run the command:./blogger_to_jekyll.rb http://{your_blog_name}.blogspot.com/feeds/posts/default2) Rename html files to markdownWhen you run the script of step 1, I will create one html file per post in the folder _posts, so need to rename them to markdown files. In order to so, you can just use this simple powershell script:Dir *.html | rename-item -newname { [io.path]::ChangeExtension($_.name, \"md\") }3) Convert content from html to markdownThis step is optional, but in my case the html produced by blogspot was complex and I notice some rendering errors in Jekyll. For that reason, I decided to convert them into markdown.For this step I used this online converter: https://codebeautify.org/html-to-markdownUse your custom domainIf you were using a custom domain for your old blog, you will have to redirect it to your new blog on Github pages.For this, you will have to go to your domain provider and configure it to point to your Github blog.First, you need to an A record to the IP addresses for GitHub Pages. At this moment these IPs are:185.199.108.153185.199.109.153185.199.110.153185.199.111.153But you can check this link in case they change in the future.Then, you need to add a CNAME record pointing to your Github URL. In my case, I’m using Google Domains and my configuration looks like this:Finally, you have to configure your custom domain in Github. For this, go to your blog repository. Them, go to Settings -&gt; Pages.In section custom domain, add your custom domain URL. This will check your configuration and it will start redirecting the traffic from your custom domain, but it might take a few minutes.Don’t forget about checking the Enforce HTTPS option.ConclusionsIn this post we have seen how to create a new blog in Github pages and migrate your content from an existing blog in Blogspot.Finally, we have seen how you can configure your custom domain to point to your new blog.This post is based on the process I’ve used to migrate my own blog after doing some researching, so I hope this can be helpful for you if you are looking for something similar.In my personal experience, it feels really good editing my articles in MarkDown language and having Git to backup my content. In addition, it has been great to play around with Ruby an explore Jekyll, which brigs us a lot of possibilities when it comes to customization of our blogs." }, { "title": "Circuit Breakers to handle failures in serverless applications", "url": "/posts/Circuit-Breakers-to-handle-failures-in-serverless-applications/", "categories": "", "tags": "patterns, cloud, architecture", "date": "2022-06-13 09:31:14 +0200", "snippet": "IntroductionWhen you are developing micro-services, you might find that there might be some dependencies which could fail or be temporary unavailable. Examples of these dependencies are calls to other services, data stores, etc. These dependencies might be unavailable for multiple reasons such as maintenance, throttling, etc.The idea behind the circuit breaker pattern is wrapping the dependency, so if there is a consistent failure, we temporary stop trying to call the dependency and instead we can have define an alternative fallback function.Normally a circuit breaker can be in three different states: Closed: we call the dependency as usual. If there are failures, we track them and we open the circuit if a defined threshold is reached. Open: instead of calling the dependency, we call the fallback function. If we don’t define a fallback function, it will fail early, without calling the dependency. This can prevent overloading an unhealthy dependent service or data store. While the circuit breaker is OPEN, if we reach a timeout, we will change the status to HALF. Half (open): if the request is successful, the status will change to CLOSED and it will start processing all incoming requests. However, if the requests are still failing, the CircuitBreaker will change to OPEN.The following diagram illustrates the different statuses and transitions:Proposed Architecture for serverless applicationsThe Circuit Breaker pattern is a well know pattern and some popular libraries such us Polly (https://github.com/App-vNext/Polly) implement this pattern. In the implementation offered by Polly, the status of the circuit breaker is persisted in memory. This works well for single instance services, but if we are running multiple instances of the service concurrently, you need to share the status of the circuit breaker, so all the instances are consistently handling the circuit breaker. This applies for AWS lambda functions or ECS if we are running multiple nodes in parallel. For lambdas, you also have to bear in mind that if the lambda gets cold, it will erase the memory. For that reason it is important to persist the status of the circuit breaker.Regarding implementations more focus on serverless, I found this implementation written in Node.js. Since our micro-services are implanted using .NET 6, I couldn’t use a node.js implementation. In addition, that implementation is meant to work at lambda level and for my scenario I wanted something more generic, which can be used by any service and any dependency.Regarding the persistence, in order to have lower latency reading/updating the state, we decided to store the status of the circuit breaker in Amazon MemoryDB, which is based on Redis with persistent storage. The object we store to persist the status is really small and we want to make the storage as performant as possible. That is the reason why Amazon MemoryDB can be a better candidate than DynamoDB for storing circuit breaker objects.Based on those principles, the proposed architecture is displayed in the following diagram, where a lambda might have two circuit breaker to manage two dependencies and persist the status in Amazon MemoryDB. In the diagram we see a lambda which has two dependencies, Amazon ElasticSearch and a external API, so we can define one circuit breaker for each one. In addition, we can also define a fallback function for each dependency, the failure handling process could be different depending on the dependency..NET 6 implementationYou can find the full source code of this implementation on my GitHub repository: https://github.com/albertocorrales/circuitbreaker-dotnet.First of all, you need to configure the dependency injectioninternal static void RegisterCircuitBreakers(this IServiceCollection services, IConfiguration configuration){ services.Configure&lt;ElastiCacheConfig&gt;(configuration.GetSection(\"CircuitBreaker\")); services.AddSingleton&lt;ICircuitBreakerFactory, CircuitBreakerFactory&gt;(); services.AddSingleton&lt;ICircuitBreakerRepository, CircuitBreakerRepositoryElastiCache&gt;();}For the persistence, we have a generic repository defined by the interface ICircuitBreakerRepository. Then we could implement other persistence storages if we want to. For this implementation, as it was discussed above, we are using MemoryDB, so the implementation of the repository is provided by the class CircuitBreakerRepositoryElastiCache.In addition, we are injecting the class CircuitBreakerFactory, so we just need to have this dependency in the classes where we are using circuit breakers.Then you can use it in your Web APIs, Lambdas, etc. For example in a Web API controller, we can it as follows:public class MyController : ControllerBase{ private readonly ICircuitBreakerFactory _circuitBreakerFactory; public MyController (ICircuitBreakerFactory circuitBreakerFactory) { _circuitBreakerFactory = circuitBreakerFactory; } public async Task&lt;IActionResult&gt; MyMethod([FromBody] RequestModel request) { var circuitBreakerOptions = new CircuitBreakerOptions&lt;ResultModel&gt;(\"circuit-breaker-id-123\", () =&gt; asyncFunction(request)); var circuitBreaker = _circuitBreakerFactory.CreateCircuitBreaker&lt;ResultModel&gt;(circuitBreakerOptions); return await circuitBreaker.Fire(); }}In the model CircuitBreakerOptions, we can configure the following properties: CircuitBreakerId (required): id for the circuit breaker. This value has to be unique, but we could also use the same value if for example we want to use the same circuit breaker in two places. (For example, if EventStore is down, it might affect projections and commands). Request (required): function to wrap under the circuit breaker. FailureThreshold: number of failure executions that should happen until the circuit is OPEN. SuccessThreshold: number of successful executions to change the status of the circuit breaker to CLOSED. Timeout: number of milliseconds to try a new request since the circuit breaker changed to OPEN. Fallback: alternative function to execute if the main function fails or the circuit breaker is OPEN.On the other hand, the CircuitBreakerModel that we persists, it is defined by the following properties: Id: unique id to identify the circuit breaker Status: current status of the circuit breaker (open, closed or half) FailureCount: number of consecutive failure requests SuccessCount: number of consecutive successful requests NextAttempt. datetime when the circuit breaker is closed and we should try next time.ConclusionsIn this article we have seen the importance of managing failures properly in your applications with circuit breakers. In addition, we discussed about the reason why multi-node applications should persist the status of the circuit breakers, so all the instances can work consistently.Finally, we have seen an implementation for .NET 6, which helps us to manage and persist our circuit breakers. The implementation is flexible and it allows us to define multiple circuit breakers and handle them independently. Furthermore, we can use MemoryDB as persistent storage or implement our own repository." }, { "title": "AWS assuming role + MFA with pulumi (and other applications)", "url": "/posts/AWS-assuming-role-+-MFA-with-pulumi-(and-other-applications)/", "categories": "", "tags": "aws, security, iam, mfa", "date": "2021-08-16 23:50:23 +0200", "snippet": "IntroductionIn AWS, managing your IAM in a centralized account is a good practice, so then you can assume a role to access to other accounts without need of duplicating users. You can find more info about this approach in this article.In addition, for security reasons, it is highly recommended to enable MFA (Multi-factor Authentication), so you need a device to generate a temporary code to get access. In the image below it is showed how assume role with MFA works, but you also have an article about MFA in this link.This approach is really useful and most of the configuration can be automated with pulumi (or other providers like terraform or CloudFormation). And once it is configured, you can switch your role and get access to other accounts with a few clicks.However, when it comes to external software such us pulumi, getting access assuming a role and using MFA might not be too straightforward. In this article we will see how this can be configured and how we can easily refresh our credentials with a PowerShell script.Getting access with assume role + MFA for pulumiIn this section, we will explain how you can setup manually your credentials, so pulumi can get access to your account by assuming a role + using MFA.Getting a token by using MFAFirst, you need to request temporary MFA credentials, using your AWS Access Key and Secret Key for your user in the account where you have your IAM user. In order to so, you can run this command, which will issue your temporary credentials valid for 8 hours (max 36 hours):aws sts get-session-token –duration-seconds 28800 –serial-number arn-of-the-mfa-device –token-code code-from-tokenWhere: arn-of-the-mfa-device: you can find it in AWS Ops account, if you click on your user (top bar), then “My security credentials”. from-token: numeric code that you can get this code with the app you are using for MFA.Setup your AWS profile credentialsThen, you need to  add two profiles. One to setup your temporary credentials and another to assume the role for the account that you want to get access.In order to setup to setup these profiles, you have to go to C:\\Users\\YourUser\\.aws\\credentials.Then, you need to add these profiles:[dev]role_arn = arn:aws:iam::#account_number#:role/#YourRole#source_profile = mfa[mfa]aws_access_key_id = #aws_access_key_id_from_get-session-token_command#aws_secret_access_key = #aws_secret_access_key_from_get-session-token_command#aws_session_token = #aws_session_token_from_get-session-token_command#So basically, when you use [dev] profile, it will assume the role you configured using the credentials that you setup in [mfa].Configure pulumi or other applicationsIn order to use your profile in pulumi, in the stack folder, you need to run this command:pulumi config set aws:profile devFor console commands or other local tools, setup your environment variable properly with PowerShell or CMD bash (*):setx AWS_PROFILE dev(*) To apply the change when you update an environment variable, you need to reset your terminal.Automating credentials refresh with PowerShellThis process is quite straightforward, but it can be a little annoying if you have to perform those manual steps each time you want to refresh your credentials.In order to make this process really easy, I’ve created a PowerShell script, which will request your MFA code and it will refresh your profile with those credentials.$MfaDeviceArn = \"#YOUR_MFA_DEVICE_ARN#\"$DurationInSeconds = 28800$OpsProfileName = \"ops\"$MfaProfileName = \"mfadev\"$MfaCode = Read-Host -Prompt 'Please, input your MFA code'$TempCredentials = aws sts get-session-token --duration-seconds $DurationInSeconds --serial-number $MfaDeviceArn --token-code $MfaCode --profile $OpsProfileName | ConvertFrom-JsonWrite-Output \"Updating your profile '$MfaProfileName' with your new temporary credentials...\"$AccessKeyId = ($TempCredentials).Credentials.AccessKeyId$SecretAccessKey = ($TempCredentials).Credentials.SecretAccessKey$SessionToken = ($TempCredentials).Credentials.SessionTokenaws configure set aws_access_key_id $AccessKeyId --profile $MfaProfileNameaws configure set aws_secret_access_key $SecretAccessKey --profile $MfaProfileNameaws configure set aws_session_token $SessionToken --profile $MfaProfileNameWrite-Output \"Profile '$MfaProfileName' updated successfully!\"Read-Host -Prompt \"Press any key to continue or CTRL+C to quit\"So now, when we execute the script, I will request our MFA code and it will setup those credentials in our profile automatically.You can find the source code in this Git gist: https://gist.github.com/albertocorrales/f05c1f84554d20e3d32222964dcfaef0 .I hope you found useful this article" }, { "title": "Performance improvement for EventSourcing/CQRS with Snapshots", "url": "/posts/Performance-improvement-for-EventSourcingCQRS-with-Snapshots/", "categories": "cqrs, dax, dynamodb, eventsourcing, eventstore", "tags": "", "date": "2021-04-12 18:12:49 +0200", "snippet": "Introduction to Event Sourcing and CQRSThe fundamental idea of Event Sourcing is ensuring that every change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself. (For more information about Event Sourcing I recommend this post written by Martin Fowler).Based on my experience working with Event Sourcing, having the sequence of events allows you to perform awesome functionalities, like time travelling or replaying your events into multiple storage technologies. However, it can be tricky for some scenarios. To explain this better, I need to introduce Event Sourcing working together with Command Query Responsibility Segregation (CQRS). If you are not familiar with CQRS, the idea behind it is splitting commands (write operations) from queries (read operations), so they can work independently. (more info about CQRS in this post).CQRS and EventSourcing in practice it is illustrated by the subsequent diagram, where we are using EventStore as Event Sourcing store, AWS lambdas for commands and queries, and ECS for projectors.In this architecture, commands will save new events in EventStore. Then Projectors will be listening for those events and they will use projections to create read replicas into any reading efficient storage. In this case DynamoDB will store our read replicas. Finally, queries will use those read replicas from dynamodb to execute read operations in a very efficient way.This architecture is very efficient and flexible, since the events can be projected into multiples data stores or used for other purposes like creating integration events. However, it brings more complexity than using a single storage with one single REST API which writes/reads. That is the reason why we can find some articles over the Internet blaming Event Sourcing, when the problem might be their lack of experience or knowledge to apply it properly. On the other hand, there is not too much good documentation for advanced scenarios. That is the main reason why I decided to share this post.Eventual Consistency The main challenge of Event Sourcing with CQRS is eventual consistency. In CQRS there is eventual consistency between commands and queries. This means that once you committed one command for a particular aggregate, if you query that aggregate, you might not have the latest version. However, occasionally you will have latest version if you keep querying it.This same effect happens in most of asynchronous, distributed or highly scalable systems, like DynamoDB, ElasticCache, etc.Something cool about Event Store is the fact that it is strongly consistent at aggregate level. That means that events for a particular aggregate will be consistent, so once you commit one event, if you query that aggregate, you will be able to get latest in a consistent way. Due to this feature, if you need to ensure that you are seeing latest, one option is querying Event Store. However, this query might not be efficient, since you need to read all the sequence of events for that particular aggregate and apply those events in order to get the latest projection of your aggregate. This might be specially inefficient when you have many events to read.Efficient reading operations for commands validationCommands need to read aggregates, in order to get the latest status, so it can validate current commands and apply the new ones on top. In order to do so, we could read from EventStore and re-hydrate the aggregate. But, like I mentioned above, this might be not efficient if we have many events to read.One option to reduce the number of events to read is creating snapshots. For example, from time to time, we could save the current status in EventStore as a snapshot event, so we don’t need to read previous events anymore. This could be a good technique to reduce the number of old events, but it could be not very suitable to do it very often, since it would increase of your data in EventStore and it would require more writing capacity.However, as we’ve seen previously, in CQRS normally you have a read replica the status of each aggregate that we update each time we process a new event. As we mentioned, that copy might be stale, but most of the time it will have latest. So we could use that copy as snapshot for our queries coming from commands, since that store is highly scalable and optimized for reading operations.In particular, the proposed architecture looks like this:So commands well query our read replica store in order to validate commands. In order to make those queries even more efficient (and save RCUs in DynamoDB), a DynamoDB DAX cache has been introduced on top of DynamoDB table. In addition, having a cache on top of DynamoDB table will also improve the performance of reading operations coming from queries and projections, so this is highly recommended. If you are using DynanoDB for storing your read replica, DAX is a really good option, because you won’t have to deal with cache sync, which can be tricky. However, this approach it is also valid for other stores and caching technologies.Adding strong consistencyBut…wait a moment! we said that what we have in our read replica could not be latest, due to eventual consistency. So, what would it happen if we are not using latest to validate our commands? Well, that could be very messy, but luckily, EventStore is strongly consistent. So, if we keep track of the version of the aggregate that we have in DynamoDB, we will be able to know if our data is stale. In order to help with this, EventStore will throw a WrongExpectedVersionException if you are working on stale data (more info here).So, for example, if we have a snapshot with version number 1000 and EventStoreDB throws a WrongExpectedVersionException telling us that latest version is 1003, that means that we don’t have latest 3 events in our read replica yet. What we can do here is reading those 3 events from EventStoreDB and apply them to our snapshot version 1000, so we would have latest and we can use it to validate our commands.The flow diagram would be something like this:In case we are having updates in parallel, the algorithm will try to catch up for defined a number of times. If we reach that limit, it will throw an exception.The idea behind this approach could be also used to cover other scenarios, like queries with strong consistency. In the same way DynamoDB allow us to query with strong consistency (more info here), we could specify in our query that we want to ensure strong consistency. In that case, we would use the aggregate version that we have in DynamoDB and then, we would check in EventStoreDB if there are any pending events. If there were any, we could apply them to the aggregate and return it.ConclusionsIn this article we have seen a method to improve commands performance by using our read replica as a repository for snapshots. This prevents having to read the whole sequence of events from EventStore each time we need to query a particular aggregate for commands validation. If we don’t have latest in our snapshot, we just need to add missing events, taking the most of the two stores.In addition, DAX was added on top of DynamoDB to improve the performance for querying these snapshots. This will improve the overall performance for commands, queries and projections without having to deal with tedious synchronization processes to keep the cache up-to-date.This technique could be also used querying with strong consistency in our query handlers." }, { "title": "Building Micro-Frontends with Single-Spa", "url": "/posts/Building-Micro-Frontends-with-Single-Spa/", "categories": "architecture, frontend, micro-frontends, react, single-spa, spa", "tags": "", "date": "2021-04-04 15:58:30 +0200", "snippet": "IntroductionMicro-services architecture is becoming trendy since it is a suitable option many projects, specially those that are complex and we want to be able spilt our business logic into different services that can be built and deliver independently. In spite of the fact that micro-services architectures are more complex, they increase the resilience of your system and increase the time-to-market delivery, which can be crucial for many companies.But it is not all about services and backend. With the introduction of single page applications (SPAs), we can build fully-functional user interfaces that can work independently in the browser. And here we should ask ourselves the same questions as for backend applications: Is my SPA becoming a huge monolith? Would I like to deliver functional modules independently? If you using micro-services, is there a relation between my services and your UI? Do we want to have functional teams that can deliver new features (frontend+backend) independently? If your answer to most of those questions is ‘yes’, you might need to consider micro-frontend architectures.What are Micro-Frontends?Here we have one of my favourite posts about micro-frontends: https://martinfowler.com/articles/micro-frontends.html. In particular, this image summarizes quite well the importance of micro-frontends:So, when it comes to micro-frontend good practices and principles, there is a list of points that I consider important in the design: Each micro-frontend should be independently developed and delivered. Micro-frontends should load efficiently by using lazy-loading. We should be able to reuse common stuff, like styles, auth, frameworks in an efficient way. Communication between micro-frontends should be allowed in a decoupled and async way, and it shouldn’t be too chatty. We should be allowed to use micro-frontends with different technologies.Single-SpaTechnology-wise, there are some popular options available (here there is an article with some of the most trendy ones).I’ve been comparing some of the frameworks of that list, and Single-Spa (https://single-spa.js.org/) seems the most suitable candidate for many reasons: It allows you to build and deliver your micro-frontends independently. In fact, they recommend having separated repositories and pipelines for each of your micro-frontends. It supports lazy loading. So your micro-frontends will be loading as you need them, instead of having a massive loading time at the beginning. It allows reusability of code and styles. In order to support this, you have multiple alternatives, like utility-modules or parcels. Data loading for shared modules is efficient. To do this, the preference is Import-maps, but Module federation is also also supported. Communication between micro-frontends is supported. Here the preference is cross microfrontend imports, but other alternatives like custom events or redux are also supported. Single-Spa doesn’t depend on any frameworks. Under the hood, it works with javascript and webpack. In addition, it offers helper libraries for the most trendy frameworks. Single-Spa is open source and the community support is strong. The documentation and the number of examples is quite good. You can mount micro-frontends dynamically with import-map-overrides. This feature is just awesome. It allows you to inject one or several micro-frontends, so you don’t have to run everything locally. There is a CLI (create-single-spa), which makes really easy to create new micro-frontends with the initial scaffolding.And here some caveats: Micro-frontends are more complex than monolith SPAs. This is not only for Single-Spa, but for every micro-frontend architecture, since distributed systems are always more complex. If you are used to working with one SPA and CRA, it can be challenging at the beginning and you might need to learn some concepts, specially about Webpack, since Single-Spa doesn’t support CRA.Single-Spa with Create-React-AppIn the previous caveats list I mentioned “Single-Spa doesn’t support CRA”. Well, that is not true. They prefer using Webpack directly, but CRA under the hood uses Webpack. So, with the proper configuration, CRA can work with Single-Spa.On the internet, you can find some articles or snippets to give you a gist of how this needs to be configured. However, I haven’t found any full examples. So I built my own example application, that you can find on my GitHub: https://github.com/albertocorrales/single-spa-cra-example.In order to build a micro-frontend with Single-Spa and CRA you need to do the following steps:1) Create a new micro-frontend with create-react-appYou can just create a new app with the following command npx create-react-app app1 --template typescriptI normally work with typescript for many reasons, but this example will also work with javascript.2) Install and configure react-app-rewiredIn order to be able to change CRA pre-configuration, you need to install a package like react-app-rewired.Execute this command:npm install react-app-rewired --save-dev3) Configure react-app-rewiredFirst of all, edit package.json to add rewired script (optionally, you can keep cra scripts):Then, in the root of your app, add a new file “config-overrides.js” with the following content:module.exports = { webpack(config, env) { config.entry = \"./src/index-single-spa.tsx\"; config.output = { ...config.output, filename: \"@company/app1.js\", libraryTarget: \"system\", }; config.plugins = config.plugins.filter( (plugin) =&gt; plugin.constructor.name !== \"HtmlWebpackPlugin\" &amp;&amp; plugin.constructor.name !== \"MiniCssExtractPlugin\" ); delete config.optimization; return config; }, devServer(configFunction) { return function (proxy, allowedHost) { const config = configFunction(proxy, allowedHost); config.disableHostCheck = true; config.headers = config.headers || {}; config.headers[\"Access-Control-Allow-Origin\"] = \"*\"; return config; }; },};4) Add single spa supportInstall package “single-spa-react” with this command: npm i -D single-spa-reactThen, create index-single-spa.tsx, which will be your entry point for your micro-frontend:import React from \"react\";import ReactDOM from \"react-dom\";import App from \"./App\";// Note that SingleSpaContext is a react@16.3 (if available) context that provides the singleSpa propsimport singleSpaReact from \"single-spa-react\";const reactLifecycles = singleSpaReact({ React, ReactDOM, rootComponent: App, errorBoundary(err, info, props) { // https://reactjs.org/docs/error-boundaries.html return &lt;div&gt;This renders when a catastrophic error occurs&lt;/div&gt;; },});export const bootstrap = reactLifecycles.bootstrap;export const mount = reactLifecycles.mount;export const unmount = reactLifecycles.unmount;At this point, your CRA app is ready to run as single-spa micro-frontend. If you run the command “npm start”, you should be able to see your javascript file on http://localhost:3000/@company/app1.js.5) Add your micro-frontend to root applicationNow it is time to create a root-config app with Single-Spa and load our micro-frontend. To do so, create a new root-config app with command: create-single-spa --moduleType root-configFinally, in your file index.ejs, replace example “@single-spa/welcome” with your new micro-frontend “@compan/app1”.  Now, if you run “npm startW for the root application, you should be able to see your micro-service working on http://localhost:9000/ConclusionsIn this post, we have seen some principles and good design practices about micro-frontends. Technology-wise, Single-Spa is a strong candidate, which offers a powerful framework, which tries to simplify building micro-frontends.In spite of the fact that Single-Spa includes a CLI what helps to create new micro-frontends, you could still using Create-React-App to run your CRA applications as Single-Spa micro-frontends, which can be a good starting point if are already working with CRA and you don’t want to move to Webpack. However, in the long term, I would suggest to getting familiar with Webpack, since sooner or later you will need some configuration and not using CRA you will have more control over it.Update 04/04/2021Recently I’ve found this plugin based on CRACO that also allows you to work with Singe-Spa and CRA. Here you have an example of this plugin configured with Single-Spa https://github.com/albertocorrales/single-spa-cra-example/tree/main/app2" }, { "title": "Managing snapshots for Amazon ElasticSearch with Dotnet Core Lambdas", "url": "/posts/Managing-snapshots-for-Amazon-ElasticSearch-with-Dotnet-Core-Lambdas/", "categories": ".net, core, 2020, Amazon, AWS, lambda, Backup, ElasticSearch", "tags": "", "date": "2020-12-13 23:30:14 +0100", "snippet": "Introduction===============It is awesome to have some useful services like ElasticSearch managed by AWS, so you don’t have to care about patching, monitoring, etc.When it comes to backup management for indexes, Amazon ElasticSearch includes automated snapshots. Automated snapshots are only for cluster recovery. You can use them to restore your domain in the event of red cluster status or other data loss. Amazon ES stores automated snapshots in a preconfigured Amazon S3 bucket at no additional charge. In particular, if you are using a higher version than 5.3, Amazon ElasticSearch takes hourly automated snapshots and retains up to 336 of them for 14 days.However, this policy might not be enough if you need to keep your snapshots for a longer 15 days, or you need to take a snapshot to create a new cluster in another region, subnet, etc. If you have different requirements to manage your snapshots, you will have to work with manual snapshots. For this part, AWS does not offer a good managed experience, since the solution is calling ElasticSearch Web API, as it is explained in this article. In fact, even for automatic snapshots, you will also have to use ElasticSearch Web API if you need to restore your index.In orther to make requests to ElasticSearch, they must be signed by AWS (more info here). For this purpose, AWS provides packages for multiple languages (Java, Python, Ruby, Node and  Go), but not for C#.In this article, we will see how we can manage Amazon ElasticSearch snapshots with dotnet core.Creating ElasticSearch snapshots with dotnet coreFirst of all, you need to cover some pre-requisites that are detailed here, as for the rest of languages: You have to create an S3 bucket. In this S3 bucket, you won’t be able to configure lifecycle policies to move to cold start or delete old data. You have to create an IAM role with access to the S3 bucket created previously. You need an IAM user and permissions to assume previous role or call elastic search with HTTP PUT.For the dotnet implementation, I created a new dotnet core AWS Lambda. In this way, I could invoke it from multiple sources. For example I can schedule a CloudWatch event to trigger it every day, every month, etc. The source code of this example can be found on this GitHub repository.First of all, this is my request object for AWS lambda input:public class Request{ public string Endpoint { get; set; } public string RepositoryName { get; set; } public string S3Bucket { get; set; } public string Region { get; set; } public string RoleArn { get; set; }}Then this is the lambda handler:public async Task FunctionHandler(Request request, ILambdaContext context){ try { var accessKey = Environment.GetEnvironmentVariable(\"ACCESS_KEY\"); var secretKey = Environment.GetEnvironmentVariable(\"SECRET_KEY\"); var signer = new AWS4RequestSigner(accessKey, secretKey); var repositoryUrl = $\"{request.Endpoint}/_snapshot/{request.RepositoryName}\"; var service = \"es\"; await RegisterS3Repository(request, context, signer, repositoryUrl, service); await CreateSnapshot(request, context, signer, repositoryUrl, service); } catch (Exception ex) { context.Logger.LogLine($\"Error performing backup: {ex.Message}\"); throw; }}As I mentioned previously, requests to ElasticSearch have signed and this is not supported by AWS. However, there is a signer for AWS that you can use from this project: https://github.com/tsibelman/aws-signer-v4-dot-net by using this NuGet package. Once we create the signer with AWS access key and secret key, we just have to register the repository and create the snapshot.In order to register the S3 repository, we have to create the request, sign it and send it with PUT http operation:private static async Task RegisterS3Repository(Request request, ILambdaContext context, AWS4RequestSigner signer, string repositoryUrl, string service){ context.Logger.LogLine($\"Register ElasticSearch Repository: {request.RepositoryName}\"); var requestBody = new { type = \"s3\", settings = new { bucket = request.S3Bucket, region = request.Region, role_arn = request.RoleArn } }; string requestBodyString = System.Text.Json.JsonSerializer.Serialize(requestBody); var content = new StringContent(requestBodyString, Encoding.UTF8, \"application/json\"); var httpRequestRepository = new HttpRequestMessage { Method = HttpMethod.Put, RequestUri = new Uri(repositoryUrl), Content = content }; httpRequestRepository = await signer.Sign(httpRequestRepository, service, request.Region); var client = new HttpClient(); var responseRepository = await client.SendAsync(httpRequestRepository); if (!responseRepository.IsSuccessStatusCode) { throw new Exception($\"Error registering repository {request.RepositoryName}.\\n\" + $\"Error code: {responseRepository.StatusCode}.\\n\" + $\"Content: {await responseRepository.Content.ReadAsStringAsync()}\"); } context.Logger.LogLine($\"ElasticSearch Repository {request.RepositoryName} successfully registered\");}Once we register the repository, we can create new snapshots in a similar way, signing the request and sending a PUT http operation to $”{repositoryUrl}/{snapshotName}” endpoint:private static async Task CreateSnapshot(Request request, ILambdaContext context, AWS4RequestSigner signer, string repositoryUrl, string service){ var snapshotName = DateTime.Now.ToString(\"dd-MM-yyyy-h-mm-ss\").ToLower(); context.Logger.LogLine($\"Create ElasticSearch Image: {repositoryUrl}/{snapshotName}\"); var httpRequestSnapshot = new HttpRequestMessage { Method = HttpMethod.Put, RequestUri = new Uri($\"{repositoryUrl}/{snapshotName}\"), Content = null }; httpRequestSnapshot = await signer.Sign(httpRequestSnapshot, service, request.Region); var client = new HttpClient(); var responseSnapshot = await client.SendAsync(httpRequestSnapshot); if (!responseSnapshot.IsSuccessStatusCode) { throw new Exception($\"Error creating snapshot {snapshotName}.\\n\" + $\"Error code: {responseSnapshot.StatusCode}.\\n\" + $\"Content: {await responseSnapshot.Content.ReadAsStringAsync()}\"); } context.Logger.LogLine($\"ElasticSearch snapshot {snapshotName} successfully registered\");}ConclusionsIn this article, we have seen the options AWS ElasticSearch offers to create snapshots. In particular, the automatic way to create snapshots is quite limited and it is not configurable, so depending on our requirements we might need to create manual snapshots.If we want to implement this process with dotnet core, in this article we have seen an example to implement a lambda which we can use go create manual snapshots. We could also extend this approach to build scheduled lambdas to remove old snapshots or any other operations.In my personal opinion, Amazon ElasticSearch should offer better alternatives, for example integration with AWS Backup." }, { "title": "How to Setup EventStoreDB on AWS EC2 with Pulumi IaC", "url": "/posts/How-to-Setup-EventStoreDB-on-AWS-EC2-with-Pulumi-IaC/", "categories": "AWS, EBS, EC2, ESDB, eventstore, IaC, Pulumi", "tags": "", "date": "2020-11-16 09:08:49 +0100", "snippet": "Introduction EventStoreDB (ESDB) is an industrial-strength database technology used as the central data store for event-sourced systems. It is available open-source to run locally on most platforms or as SaaS through Event Store Cloud. Currently their SaaS version is under development, so if you want to run it on your cloud, you will have to set it up.One option is docker, since ESDB is available as docker image on Docker Hub. However, as Greg Young explains in this issue, it might be not the best option when it comes to performance, due to the extra virtualization layer. For the same reason, we don’t usually use docker images for other database engines like SQL Server, MongoDB, etc.Then, if you want to run ESDB on cloud, in particular AWS cloud, EC2 + EBS is the most reasonable option.In this post, I would like to share how you can configure ESDB on EC2+EBS easily with Pulumi, which will allow you to automate this process following the best practices.Creating EC2 with PulumiIf you are not familiar with Pulumi, I recommend to start having a look at this documentation, where it is explained how to setup a new project for AWS.Creating a new EC2 in Pulumi is really straightforward. You just need to define your instance like this:In that code, you can specify the following parameters: instanceType: type of EC2 instance that you want to use keyName: key pair that you will use to connect with SSH ami: AMI you want to use. For example AMI “ami-06fd8a495a537da8b” is to use Ubuntu Server 20.04 LTS vpcSecurityGroupIds: the security group you are going to use. Here for ESDB you might need to enable ports 1113 and 2113. For SSH you will need to enable port 22. userData: this is the script to setup ESDB. We will see this more in detail later. tags: your AWS for this resource. It is highly recommended to tag all your AWS resources with meaningful tags.Setup EventStoreDB on EC2First of all, lets have a look at ESDB config file.# PathsDb: /data/eventstore/dbIndex: /data/eventstore/indexLog: /data/eventstore/log# Certificates configurationCertificateFile: /data/certs/node1/node.crtCertificatePrivateKeyFile: /data/certs/node1/node.keyTrustedRootCertificatesPath: /data/certs/ca# Network configurationHttpPort: 2113ExtTcpPort: 1113EnableExternalTcp: trueEnableAtomPubOverHTTP: true# Cluster configClusterSize: 1# Projections configurationRunProjections: SystemIf you want to know more about ESDB, I recommend you to visit this guide, which is quite good. Here I normally configure ESDB data and certificates in folder /data, which is in an independent EBS, but I will skip that part in this post, in order to simplify.Now, in our EC2 userData we need to follow these steps:1) Install EventStoreIn this step we install EventStore repository and we install the latest version, which currently is version 20.6.1.# Install EventStorecurl -s https://packagecloud.io/install/repositories/EventStore/EventStore-OSS/script.deb.sh | sudo bashsudo apt-get install eventstore-oss=20.6.1-2   2) Update EventStore ConfigIn this step, we overwrite eventstore.conf file with the configuration we saw previously.# Update EventStore Configsudo echo \"${getEventStoreConfig()}\" &gt;| /etc/eventstore/eventstore.conf3) Create /data folder and add permissions to eventstoreThen, we create folder /data and we assign permissions to eventstore, so the service will be able to own that folder.# Create /data folder and add permissions to eventstore    cd /    mkdir data    sudo chown eventstore data    sudo chgrp eventstore data4) Create certificatesIn this step, we create CA and Node certificates for SSL connection with es-gencert-cli, which is a software created by EventStore to simplify this process. You could also use OpenSSL if you wish.# Create certificates    cd /data    sudo mkdir certs    cd certs/    sudo wget -c https://github.com/EventStore/es-gencert-cli/releases/download/1.0.2/es-gencert-cli\\_1.0.2\\_Linux-x86\\_64.tar.gz    sudo tar -xzvf es-gencert-cli\\_1.0.2\\_Linux-x86\\_64.tar.gz    sudo ./es-gencert-cli create-ca    sudo ./es-gencert-cli create-node -out ./node1 --dns-names \\*.example.com    find . -type f  -name '\\*.crt' -o -name '\\*.key' -print0 | sudo xargs -0 chmod 666### 5) Run EventStore Finally we enable and start eventstore service.    # Run EventStore    sudo systemctl enable eventstore    sudo systemctl start eventstoreYou can find the full example source code on this GitHub repository: https://github.com/albertocorrales/eventstoredb-aws-ec2ConclusionsIn this post, we have seen how we can setup EventStoreDB on AWS EC2 with Pulumi, as IaC framework. If you want to know more about these technologies, I suggest to visit EventStore documentation and Pulumi documentation." }, { "title": "Code coverage report generation for dotnet core applications", "url": "/posts/Code-coverage-report-generation-for-dotnet-core-applications/", "categories": ".net, core, 2020, code, coverage, quality, ReportGenerator, testing, unit, tests", "tags": "", "date": "2020-10-12 15:48:45 +0200", "snippet": "In “Working Effectively with Legacy Code” Michael Feathers introduced a definition of legacy code as code without tests, since a code without tests is difficult to maintain, extend and evolve. It doesn’t matter if you are using the latest technologies, if you don’t test your code, it will hard to change without breaking anything, which will make it rigid and difficult to maintain. Remember that unit tests are the base in the testing pyramid.In order to ensure that you are covering with unit tests the most important part of our application, code coverage can be helpful to detect where you have lack of tests.Code coverage is a tool offered by products like ReSharper or the Enterprise version of Visual Studio. However, these products are not free and you might not want to pay for this essential feature.But if you are working with dotnet core, it is quite easy to get a code coverage report thanks to ReportGenerator package.In order to generate your tests, first you need to install ReportGenerator in your machine with this command:dotnet tool install -g dotnet-reportgenerator-globaltoolThen you can run your tests and create coverage reports by using this command:dotnet test --collect:\"XPlat Code Coverage\"Finally, you can use reportgenerator command to generate a report with actual coverage in your code:reportgenerator \"-reports:./**/coverage.cobertura.xml\" \"-targetdir:./coverage\" \"-reporttypes:Html\"And that is all. Handy, right?. In addition, since I like to make it even more easy to execute, I’ve created a .bat file, which execute my tests, generates a coverage report, and opens chrome to display the report. As a result, I can update my coverage report with double-click. This is the code for my script:dotnet test --collect:\"XPlat Code Coverage\"reportgenerator \"-reports:./**/coverage.cobertura.xml\" \"-targetdir:./coverage\" \"-reporttypes:Html\"start chrome ./coverage/index.htmlIn order to provide an example, I’ve created this GitHub project with some tests: https://github.com/albertocorrales/code-coverage-reportWhen I execute my coverage.bat script, my browser is automatically opened displaying this coverage report.In our report we can see the coverage for each of our files and different metrics like line coverage, branch coverage, etc.I hope you found helpful this article." }, { "title": "Making DDD and CQRS Microservices easy with dotnet templates", "url": "/posts/Making-DDD-and-CQRS-Microservices-easy-with-dotnet-templates/", "categories": ".net, core, architecture, cqrs, ddd, microservices, useful, Visual, Studio, web, services", "tags": "", "date": "2020-08-03 20:32:14 +0200", "snippet": "IntroductionIf you are working with Microservices, you might have noticed that you are repeating over and over the scaffolding for your new microservices. In order to make easier the creation of new microservices, you can build your own custom dotnet core template.Apart from saving time, building a custom template will help to promote good practices, a proper architecture, right dependencies, etc. With this purpose, I have built my custom template, so it can help us to create easily new microservices based on CQRS and DDD, without having to build the same scaffolding each time we have to create a new service.Microservice architectureIn microservices architecture, each service can be different, according to the problem it has to solve. However, it is likely that you have a reference architecture that will provide some consistency and it will promote good practices.In our case, the main patterns that drive our microservice architecture are Domain Driven Design (DDD) and Command and Query Responsibility Segregation (CQRS). If you don’t know these concepts, Martin Fowler has two good articles for DDD and CQRS.In next diagram, we can see a summary of one our more standard microservices, where we deploy Command and Query as independent AWS lambda functions. On the one hand, Command API will publish events to EventStore. On the other hand, Query API will read from DynamoDB database. This pattern decouples reading and writing operations, offering high scalability and performance for reading operations, which are the most frequent.Finally, we have a worker service subscribed to EventStore, so each time it receives a new event, it projects that event into DynamoDB. This service is running as docker container and it is orchestrated by AWS ECS.If we have a look at the architecture decomposing it by layers, we can observe that it is using DDD as reference. So basically, the domain layer will be the core of the application, containing business logic and domain events. In addition, we have a data layer to convert these domain models to data models. And there is also an application layer to handle application logic and using data layer with Dependency Injection (DI). On the top, we have distributed services layer, where we have Web APIs or Service Workers. Finally, we have a platform layer to share cross cutting code with our microservices.Building a custom dotnet core templateIf we translate the architecture patterns we explained in previous section into a dotnet core solution. We will have multiple projects to separate concerns between different layers. In addition, we will have different projects for different applications and also for tests.In order to build our reusable template, the first step is building a generic solution that we can use as reference to build new microservices. In particular, I’ve created this solution named MicroserviceTemplate, that contains all the needed scaffolding to implement patterns like DDD and CQRS out-of-the-box. As you can see, there are different folders in the first level to organize the code by DDD layers. So we have distributed services, application, domain and data. Furthermore, these folders have been numbered to organize them in the proper order if we use DDD architecture diagram as reference.In addition, we have different folders to separate Command Web API, Query Web API and Projector worker service. Finally, application layer has also been divided into three projects that belong to command, query and projector respectively.Test projects follow similar structure, but they have been hidden to simplify. However, you can find the full example on my GitHub repository: https://github.com/albertocorrales/dotnet-template-ddd-cqrs.In this template, it is important to use the same namespace for all projects, since we will be able to replace it later when we use it as template. In my example, I have always used “MicroserviceTemplate” as namespace.Once you have created your reference solution, you have to configure your template. To do it, you can use Sidewaffle Creator extension, or you can just use the ones I shared on GitHub and modify them. What you need to define your template are these files that you can find in .template.config folder https://github.com/albertocorrales/dotnet-template-ddd-cqrs/tree/master/template/.template.config. This configuration should be in the same folder that you have your solution.Additionally, you could define multiple templates in one package. For example, Amazon uses this technique to distribute multiple lambda dotnet core templates within the package Amazon.Lambda.Templates.Publishing and using a custom dotnet core templateOnce you have defined your custom dotnet template, you can pack it as NuGet package and use it locally or you can publish it into a NuGet feed.For both scenarios, the first step is defining your nuspec file, which will be used to generate your NuGet package. In this file, you configure parameters like the package name and version, the files to be added to the package, etc.Publishing to a local packageIn order to create your package locally, open a terminal where you have your nuspec file and execute this command:This is going to generate a file MicroserviceTemplate.1.0.0.nupkg that contains your template as NuGet package.In order to install your template use the command:When this command finishes, you will see a new template in your list of templates named “MicroserviceTemplate”.In order to use this template to create a new microservice, in the folder you can to create your solution use this command:Now you will see that your new solution has the same architecture you defined in your template, but all the namespaces and projects have been renamed to use “MyCoolMicroservice” as namespace:Publishing to a private NuGet feedIn order to publish into a private NuGet feed, the best option is using CI/CD. In my case, I’m using Azure DevOps and I have a release defined as:Since the package is in a private feed you need to authenticate. However, –interactive options is not supported yet by dotnet new –install command, so you will need to add your private NuGet feed. More info about this in this issue: https://github.com/dotnet/templating/issues/1747.In order to access to your feed, you need to generate a Personal Access Token (PAT). Here there is more info about how to generate a new PAT.Once you have your PAT, execute the following command: nuget source Add -Name “\" -Source \"\" -UserName -Password Now, to install your template from your private NuGet feed, use this command: dotnet new –install MicroserviceTemplate::1.0.0 –nuget-source “\"Once you have installed your template, you can use it with dotnet new command to create new solutions based on your template.ConclusionsIn this article, we have seen how custom dotnet core templates can help you to save time and promote good architectural practices when you have to create new microservices. We have used as example a microservice based on architectural patterns DDD and CQRS to define our template.Finally, we have explained how we can use this template to generate a NuGet package to share the template and use it with dotnet new commands.Remember that this is just one example to illustrate the concept, but you could add more useful stuff in your template, like build definitions, IaC code, etc. In addition, you could define multiple templates in one package to cover the most used patterns that you might need for your microservices.You can find the source code related to this post on my GitHub repository: https://github.com/albertocorrales/dotnet-template-ddd-cqrs" }, { "title": "Infrastructure as Code with Pulumi, AWS and Azure DevOps", "url": "/posts/Infrastructure-as-Code-with-Pulumi,-AWS-and-Azure-DevOps/", "categories": "AWS, Azure, DevOps, CI/CD, IaC, microservices, Pulumi", "tags": "", "date": "2020-07-12 22:41:48 +0200", "snippet": "IntroductionI still remember when I had to ask IT to have a new server. After that, I had to wait and pray for having the server with the corresponding permissions, software, etc. That is that Martin Fowler describes in his article about IaC as the Iron Age. Having lived that experience makes me feel a little old, but at the same time I feel lucky for having experienced the evolution, specially with the introduction of the cloud, or the Cloud AgeSince the cloud showed up, the dependencies between developers and IT have been reduced drastically, until the point that most of the work related to infrastructure and operations can be handled by developers.Creating infrastructure on the cloud is really easy, compared to having to setup your on-premise machines. Most providers offer their UI, where you can select what you want to create and which parameters you want to configure. I guess this is how we all stated the first time we used the cloud. However, when your infrastructure grows and you need to maintain multiple environments, you have probably seen how tedious can be the manual approach to create and maintain cloud resources.Infrastructure-as-Code to the rescueImagine each member of the team creating resources manually as they are needed, or trying to replicate an environment. In summary…it can be a big mess.Fortunately, Infrastructure-as-Code (IaC) came to the rescue. IaC is the infrastructure management using code and the same versioning as DevOps teams use for source code. Following the same principle that source code will generate always the same binaries, IaC will generates always the same infrastructure. IaC can be extremely powerful if we use it in conjunction with continuous delivery.In fact, the value of IaC can be broken down into three measurable categories: cost (reduction), speed (faster execution) and risk (remove errors and security violations).But there was a problem. To implement Infrastructure as Code….there was no code! There were templates. In particular YAML templates. YAML templates are good for small config files, because they are quite readable for small files with few levels of tabulation spaces. But infrastructure can require a significant amount of YAML that can be really hard to read. And that is not the worst part, the worst part is having to write that code.I know YAML is trendy these days, but in my experience, it is really hard to create and maintain. I would even prefer JSON, because at least you won’t have tabulation space errors, which are quite frequent in YAML.Some of the most popular IaC products based on templates are Microsoft ARM, AWS CloudFormation and Terraform.However, there are other more recent products that are trying to implement Infrastructure-as-Code with real coding languages. The most popular is Pulumi, and that is the technology I’ve been using to build my microservices architecture.In addition, if you work with AWS, you have the possibility to use AWS CDK https://aws.amazon.com/cdk/. AWS CDK also allow you to work with nice programming languages like typescript or C#, but it will generate an AWS CloudFront template from your code. However, it is good to see that main cloud vendors are starting to develop real infrastructure as code tools, because that means they are being aware of the gap they have in their traditional IaC tools based on YAML.Building Microservice Architectures with Pulumi on AWSPulumi is an open source project that supports multiple programming languages, such as: .Net Core (C#, VB, F#), Go, Node.js (JavaScript, TypeScript) and Python. In contrast to other template language based alternatives, Pulumi is not free for commercial purposes. In spite of that fact, I think it is a good product and paying for it is worthy, but bear that in mind.If you are a developer, Pulumi will be really straightforward for you. You just need to choose your favourite language and start writing resources for your infrastructure as if they were classes in a normal program.In this post, I’m not going to write any examples, because there is a great repository with many examples (https://github.com/pulumi/examples) and their documentation is great. However, I would like to mention some good practices that I found useful while I was developing our microservices architecture with Pulumi.Use small stacksIn our microservices architecture we have multiple services per domain. We could have built one stack per domain or even one stack for the whole infrastructure. But having smaller stacks will be easier to maintain and operate, in case you have to destroy one of those stacks. For me the ideal unity for a stack is something that makes sense as logic unity. For example, if your microservice uses some Security Groups, a Load balancer and one ECS service, those three would belong to the same stack.In order to organise better your stack, you can also user classes or creating cloud components. Then, you can decide whether you want to distribute them as an NPM/NuGet package or not.Use dependent stacksPulumi has a cool feature called Inter-Stack Dependencies. This feature makes easier to share the resources you created in one stack with other dependent stacks. For example, you can share resources like your VPC in the stack “my-example/infra/dev”:Then you could use that vpcId you created in other stacks in this way:I wouldn’t overuse this feature if you don’t want to end up with many dependencies, since it might be difficult to track and maintain. But it is a feature nice to have into consideration.Use tagsTags are extremely useful to categorise resources on AWS, but it can also be really handy to organise your stacks, depending on your domains, team, environments, etc.In order to create a tag, you just need to write:Use Pulumi secretsProbably, you will have in our stack some variables that are secrets and you don’t want to push into your repository as plain text. For those, Pulumi includes a build-in secrets management system.In order to work with secrets, you just have to use the flag –secret when you add new configuration.That command will generate an encrypted string for you. Also, you will see it with asterisks in your pulumi management page and every time your program print it.Pulumi also supports external encryption providers for the main cloud vendors: https://www.pulumi.com/docs/intro/concepts/config/#available-encryption-providersUse pulumi-awsx packageIf you are working with AWS, Pulumi provides two packages: pulumi/pulumi-aws and pulumi/pulumi-awsx. Pulumi-aws package contains all AWS components in a similar way they exist on AWS. However, AWSX is an upper layer that simplifies quite a lot the work, providing more high-level components, with many options configured out-of-the-box for you.This package can be really convenient to save time and complexity, if you don’t need to define all the details for every resource.However, this package is not still available for all languages and that was one of the main reason why we decided to use Typescript over C#.CI/CD with Azure DevOps and PulumiOnce you have your infrastructure written with Pulumi, you will start to see the benefits. For example, in order to create a new environment, you just need to write your config for those stacks and execute “pulumi up”. As simple as that. And with the guarantee that your infrastructure is well configured and no human errors were made.But you will have the full potential of IaC when you combine it with Continuous Integration / Continuous Deployment (CI/CD). In that moment, creating new infrastructure will mean developing the new pieces in your Pulumi code, creating a PR and those changes will be made for you in your Cloud provider. In addition, the delivery of those changes will be promoted to your environments in the same way you normally deliver your application.In order to offer integration with CI/CD, Pulumi supports most of the well-known vendors: https://www.pulumi.com/docs/guides/continuous-delivery/.In my particular case, I use Azure DevOps for CI/CD. In order to support Azure DevOps easily, there is a task that you can use in your pipeline: https://marketplace.visualstudio.com/items?itemName=pulumi.build-and-release-taskWhen you use Pulumi task on Azure DevOps, you will need to create a token in our Pulumi account. Then, you have to set that token as environment variable with name: PULUMI_ACCESS_TOKEN.If you are working with AWS, you will need to setup your AWS credentials too. I normally do that step with bash task like this, where AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are my AWS credentials that I have as encrypted environment variables:Finally, I can use my Pulumi task to deploy my IaC ConclusionsIn this article we have seen why Pulumi can be great tool to build your IaC and some good practices to improve your IaC when you are working with this technology. In my opinion, Pulumi is a great tool that makes really simple writing infrastructure as code, compared to template language approaches. It supports some trendy programming languages and it is multi-cloud." }, { "title": "Graph databases with AWS and dotnet core", "url": "/posts/Graph-databases-with-AWS-and-dotnet-core/", "categories": ".net, core, AWS, graph, databases, gremlin, neptune", "tags": "", "date": "2020-06-15 13:01:51 +0200", "snippet": "IntroductionGraph databases are NoSQL databases that store data using graph structures. In other words, your data will be stored as nodes and edges, which represent your objects and their relationships. Graph databases were created to address the limitations of relational databases when you need to create relationships and consume then in an efficient way.Some use cases where graph databases are a good choice are: fraud detection, retail, risk and compliance, social networking, knowledge graph, recommendation engines, life sciences, telecommunications, etc.In my particular use case, we wanted an efficient data storage to understand the relationship between entities that we have registered in our system and extract that knowledge for multiple purposes like fraud detection, retail, risk and compliance, etc. In addition, as our product is could, we analysed the options that AWS offers us to implement graph databases with dotnet core.Alternatives to work with AWS and dotnet coreIn spite of the fact that graph databases are a quite innovative concept for most companies, there is a big community and plenty of engines, languages, etc. When it comes to AWS, you can run whatever technology you want, but we focused on the most popular engines and languages.Neo4jNeo4j (https://neo4j.com/) is a graph database management system developed by Neo4j, Inc. Neo4j is the most popular graph database engine and it was released in 2007.Advantages: Neo4j uses Cypher as language, which is a language easy to understand. There are some nice clients to work with Neo4j, which makes easy to visualize what you have in your database. The community and the documentation for Neo4j are really good. Cyper is supported by many languages (javascript, phyton, .NET, Java, etc).Disadvantages: Cypher was created by Neo4j, so you can’t use it with other engines. There is not SaaS offering on AWS for Neo4j, although there is a cloud service out of AWS (https://neo4j.com/aura/?ref=cloud) If you decide to run it on AWS, you have to run an EC2 instance or a docker container, so you will be responsible for configuration, security, backups, maintenance, scalability, etc.Amazon NeptuneAmazon Neptune (https://aws.amazon.com/neptune/) is the service offered by AWS to work with graph databases. Amazon Neptune is a relatively new service (compared to Neo4j) that was released in 2017.Advantages Amazon Neptune is a AWS SaaS product and you will have some important features out-of-the-box, such as: highly availability read replicas point-in-time recovery continuous backup to Amazon S3 replication across Availability Zone etc Amazon Neptune is based on standards. It supports two of the most popular languages: Gremlin (http://tinkerpop.apache.org/gremlin.html) and SPARQL (https://www.w3.org/TR/rdf-sparql-query/). These languages allow you to work with Neptune or other engines. Gremlin is supported by a huge variety of languages, such as .NET, java, typescript, scala, phyton, go, elixir, php, etc. Gremlin is supported by many graph systems like Microsoft CosmosDB, Neo4j, Titan, Hadoop, IBM Graph, Alibaba Graph Database, etc. More info: http://tinkerpop.apache.org/providers.html#data-system-providersDisadvantages Amazon Neptune doesn’t include any tools to visualize your data. If you use gremlin you can use graphexp (https://github.com/bricaud/graphexp), but it is quite basic and a little buggy, since it is an open source project with few contributors. The learning curve for Gremlin language can be a little harder at the beginning as there are less resources, but fortunately there is a good documentation available http://tinkerpop.apache.org/docs/current/reference/ and this ebook with many examples: https://kelvinlawrence.net/book/Gremlin-Graph-Guide.html Amazon Neptune cluster is not accessible if you are outside of the VPC where you deploy it. There are options like deploy an EC2 or a network load balancer, but it would be nice if you could get access without paying for additional resources. That is something that AWS could address by allowing IAM access control, as for other database engines.Gremlin and dotnet coreTaking into account pros and cons, we decided to have a try to Neptune with gremlin language. Firstly, because it is an AWS fully managed service and one of our principles is trying to invest our time building business value, instead of maintaining the cluster, the storage, etc. In addition, using gremlin as language will give us the possibility to use other platforms like DynamoDB, CosmosDB or even Neo4j.Run gremlin locallyIn order to run gremlin locally, I strongly recommend using docker. In fact, here you can find a docker compose which will run gremlin server and grephexp as UI, which will be handy to visualize your database: https://github.com/designfrontier/gremlin-localOnce you run command “docker-compose up”, you should be able to see your empty data base on http://localhost:8183/Using Gremlin.NETIf you are using dotnet core, the most popular plugin to work with gremlin is Gremlin.NET: https://github.com/apache/tinkerpop/tree/master/gremlin-dotnetTo work with Gremlin.NET you just have to install Gremlin.NET NuGet package in your dotnet core application. nuget install Gremlin.NetThen, you need to create your server, your client and your traversal, which will allow you to perform different operations with your graph database.var server = new GremlinServer( \"localhost\", 8182, false);var client = new GremlinClient(server);var g = Traversal().WithRemote(new DriverRemoteConnection(client));In case you are working with Neptune cluster, the hostname will be the DNS for your cluster and SSL will be enabled.With Gremlin.NET package, you can write queries with string format or you can use the fluent API. Here you can find an example to see how you can execute string queries: https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-dotnet.htmlHowever, fluent library will provide you a better experience. To illustrate this, I’ve created a very simple REST Web API where you can register, remove or query people. Each person in the graph database will be represented by one vertex or node. In addition, you can create relationships between people, which will be represented by edges.Once we have our transversal configured, we could create a vertex by doing:[HttpPost] public async Task Post([FromBody] Person person) { await _g.AddV(\"person\") .Property(\"key\", person.Id) .Property(\"name\", person.Name) .Promise(t =&gt; t.Next()); }As you can see, you add a vertex by using “AddV” instruction and then you define the label for that vertex. Then, you can define your custom properties for that vertex. For each vertex property, you can define an array of values.Finally, you resolve the promise to have an async execution of your action, which is strongly recommended if you want to have a better performance. You might notice the “Id” property has been defined as “key”, because “id” can cause conflicts with some engines which use that property to auto-generate internal ids. However, for Neptune you can overwrite you could use “id” property to define your own id without any conflicts.In order to define an edge, you can do:[HttpPost] public async Task Post([FromBody] Relationship relationship) { var source = await _g.V().Has(\"key\", relationship.SourcePersonId).Promise(t =&gt; t.Next()); var target = await _g.V().Has(\"key\", relationship.TargetPersonId).Promise(t =&gt; t.Next()); await _g.V(source) .AddE(relationship.Type) .To(target) .Property(\"key\", relationship.Id) .Property(\"sourceId\", relationship.SourcePersonId) .Property(\"targetId\", relationship.TargetPersonId) .Property(\"type\", relationship.Type) .Promise(t =&gt; t.Iterate()); }In this code, you are getting source and target vertexes, and then creating a connection between them by using “AddE”. Then, you can add your custom properties. In this case, each property will have only one value.As a example, by using this API  I’ve created a simple graph with four people and different relationships between them. In Graphexp it would look like this:You can find the source code in my GitHub repository: https://github.com/albertocorrales/gremlin-dotnet-example.ConclusionsIn this article we reviewed the most popular alternatives to use graph databases on AWS, analysing pros and cons. In my use case, we decided to have a try to Neptune with Gremlin, since it is a fully managed service and Gremlin is a standard language supported by multiple engines. However, Neo4j is also a great technology with a good community and support.Finally, we saw an example with dotnet core. This example allow us to register people and create relationships between them using graph databases with gremlin." }, { "title": "From OpenAPI to source code with CI/CD", "url": "/posts/From-OpenAPI-to-source-code-with-CICD/", "categories": "CI/CD, NPM, NuGet, OpenApi, typescript, Web, API, web, services", "tags": "", "date": "2019-12-04 23:17:06 +0100", "snippet": "IntroductionIn the era of Microservices, you are probably designing or consuming one or more Web Services. As a result, you might be aware of the importance of having a good definition of your web services, since that is what clients and developers will use to know how a web service can be used.For that reason, you should have a clear and easy way to define and understand your Web API. In order to help you to make a good definition we have REST (Representational state transfer), which is a software architectural style that defines a set of rules to create Web Services. I’m not going to go into details about RESTful, because there is enough material for a full post and it is out of the scope of this post. But I might write another post about REST, since even nowadays I see some crazy stuff when it comes to Web APIs design.Well, imagine we already have a really awesome definition for the resources of our Web API, and your clients are going to be really excited to use it, but…how do I let them know which are my endpoints, models, message errors, etc?In order to provide a way to define our REST APIs, in 2011 Swagger created the first version for RESTful APIs specification. Over the subsequent years, Swagger specification has been evolving. In 2017, the Open APIInitiative, under the sponsorship of the Linux Foundation, published the OpenAPI Specification v3.0.0, which is the consequence of the evolution of RESTful API definition and it is considered the standard to define REST APIs nowadays.In this article we will see the benefits of having an OpenAPI contract and all the possibilities that it can offer you, such as generating source code automatically form your definition and distributing it among your clients.Generating your OpenAPI contractIf you are working with Web Services, I strongly recommend that you follow a contract-first approach. No matter if your consumers are external to your organisation, if you define the contract first, you will have several benefits: Separation of concerns. Avoid blocks between teams (frontend and backend). Good and consistent communication between teams. A clear definition of your API from the beginning. Easy documentation. Consistent API models. Reusability and code generation. Easy versioning control.In order to define your OpenAPI contract, you can use multiple tools. However, I would like to recommend two of them, one to work locally and another to work in the cloud.Generating your contract from VS CodeIf you prefer working offline, there is a nice extension for VS Code to edit your API definition called OpenAPI (Swagger) Editor (https://github.com/42Crunch/vscode-openapi).By using this editor, you will have a GUI to edit your contract and you will have your contract edited in real time.By using VS commands, you will have also a list of commands that can help you to add new elements into your OpenAPI contract:Finally, one cool feature added recently is the security audit, which can generate a report from your contract, where all the security issues detected are reported for you:Generating your contract on the cloudIf you prefer working on the cloud, the tool that I like the most is apicurio (https://studio.apicur.io), which is also open source https://github.com/Apicurio/apicurio-studio. With apicurio, you will be able to define and modify your OpenAPI contract from your favourite browser. In addition, you can invite collaborators and have a history with the changes that are made to your contract.Apart from helping you with your API definition, apicurio can generate documentation from your OpenAPI contract, which can be especially interesting if you are working with third party clients or business people.If you want to know more about tools for editing your OpenAPI contracts, I suggest having a look at this URL: https://openapi.tools/#gui-editorsGenerating Code from your contractApart from having a clear definition of what your clients can expect from your API, OpenAPI contracts offer a lot of additional advantages. One advantage that I found extremely useful is generating code from your contract.During the latest months, I’ve been working in a project based on React and at the beginning the client code to consume APIs was implemented manually. As a consequence, when there was change in a service, you had to find out what part of the code you had to change to update to a newer version of the contract.To make things worse, the project was implemented in JavaScript. In other words, mistakes during manual updates of your API code could lead to run-time errors. Considering the fact that our client application was going to consume multiple services in constant evolution, the manual coding approach didn’t seem a good option.One of the best contribution that we had was adding Typescript support. By using Typescript (as I analysed in my previous post Benefits of using TypeScript in your project), one of the greatest advantages is having errors in compilation time. As a result, in spite of having manual updates for contracts, our changes were consistent through the application and real-time errors were reduced to a large extent.With this scenario, another improvement that I introduced is generating our code from OpenAPI contract. By doing so, we are saving a large amount of time, as no manual changes in our code are required anymore. In addition, when we update a contract for any API, we will have feedback about compilation errors if we have breaking changes, which leads to a smooth and safer maintenance of our client code.In order to generate your code from an OpenAPI contract, there are multiple alternatives. Particularly, we are using openapi-generator (https://github.com/OpenAPITools/openapi-generator), due to its multiple possibilities to be used by almost any existing framework or language.I personally use openapi-generator as an npm package, so the first step is to install the package globally:npm install @openapitools/openapi-generator-cli -gNext, I can configure the generator to generate typescript client code with the following command:openapi-generator generate -i MyOpenApiContract.json -g typescript-fetchDistributing your API definition with CI/CDAt this point you already know how you can generate code from your OpenAPI contract. However, in order to use that code for multiple projects, as it is likely to happen for Web APIs, you might want to pack the generated code and publish it, so all your clients will be able to get the latest version by using your packages.In order to make this process really easy, I came up with the idea of creating and publishing an NPM package from the source code generated by openapi-generator. Furthermore, I included CI/CD, so I will publish automatically a new version of my package each time I modify my contract.In my case, I’m using Azure DevOps, so I have a repository where I store my contract and my build will be triggered always that a new change is detected. During the build process, I will execute some commands that I’ve defined in my file package.json:{ \"name\": \"my-api\", \"version\": \"1.0.0\", \"private\": false, \"repository\": { \"type\": \"git\"If we have a look at my build pipeline, we can see the first two tasks, which are intended to read the version from the contract definition file and write it into my package.In the third step, I will install all the packages that I need to execute the following steps.Next, I generate my code by execute the command “npm run api-gen”. As you can see in the package.json definition, this step is invoking the library openapi-generator and subsequently, it is doing a replacement. This replacement is done because we are working with a more recent version of Typescript.Next, during the step “Transpile” I execute the command “npm run tsc”, which transpiles the typescript code into javascript + ts definitions. Finally, I publish the results of the transpilation as a build artifact.After a successful build, my release definition basically reads version of the package, concatenates the number of the build, and publishes the package as an NPM package.ConclusionsIn this article, we introduced OpenAPI and the importance of using contracts to define your Web APIs.Subsequently, we have saw how you can generate your typescript client source code from an OpenAPI contract, which can be extremely useful and can save you time developing and maintaining that code manually.Finally, we analysed the idea of publishing packages from your OpenAPI contract automatically, by using CI/CD. As a result, our clients will be able to get new versions of your Web API and consume it easily.This process can be extended, since openapi-generator supports multiple languages and server code generation, so there are a lot of scenarios that this process might cover when it comes to automatise source code generation. For example, here I showed how we can generate an NPM package, but I’m also using this process to generate and publish a dotnet Nuget package." }, { "title": "React Rally 2019", "url": "/posts/React-Rally-2019/", "categories": "2019, conference, react, reactrally", "tags": "", "date": "2019-08-26 08:21:26 +0200", "snippet": "If you work with React, you might be interested in watching #ReactRally 2019 conference:- Day 1: https://youtube.com/watch?v=dvtfNpt75aA- Day 2: https://youtube.com/watch?v=8dKljTMDGu0Here you can find the schedule: https://www.reactrally.com/scheduleEnjoy it!" }, { "title": "Git and branching workflows", "url": "/posts/Git-and-branching-workflows/", "categories": "CI/CD, git, Gitflow, product, delivery", "tags": "", "date": "2019-07-19 09:26:48 +0200", "snippet": "IntroductionWhen you are working in a software project, one of the most important aspects is your branching strategy. If you are using GIT (and if don’t, probably you should consider it), there are several strategies, such as Gitflow, Github flow and Gitlab flow, etc.In this article, I will try to introduce them and analyse pros and cons for each one. Spoiler alert, there are advantages and disadvantages for all of them, so taking that into account, you should consider which one fits better for your project and team.Environments and software life-cycleFirst of all, in order to understand why we need a particular branching strategy, we need to have clear which is the relationships between versioning, environments and software life-cycle.Normally, in software development, we can find three environments, where we will be deploying our versions for different purposes. These can be more than three, but lets focus on the basic pipeline. I will call those environments “Dev”, “Test” and “Production” (although there are other conventions like “Dev”, “Staging” and “Stable”).Dev Environment Dev is the environment for development. If we are using CI/CD, each commit should be automatically deployed here. Normally, in Dev environment we can deploy and test functionally that we are currently developing for next version.Test EnvironmentTest environment is used when we have a particular version, which we are planning to release. In test environment, we should execute more intensive testing and fix the issues we find to make this release stable for production. In Test environment QA’s will have a very relevant role. This environment is also known as Pre-Production, Release, QA, etc.Production EnvironmentProduction environment is the environment that our final users will see and use. For this reason, if we find a bug in this environment and it is really critical for our users, we should hotfix that bug to sort it out as soon as possible.So all versions that we deliver follow the flow Dev -&gt; Test -&gt; Production.GitflowGitflow is the most known branching strategy, which tries to cover all the scenarios that we can find during our software life-cycle.In Gitflow there are two branches with infinite lifetime: Develop: in this branch, the team integrate all new developments and fixes for future versions. Master: this branch contains the versions which are ready to deploy to production.And there are also some temporary branches. Release: this branch will contain new Releases in order to make it stable before going to production. When a new release is stable, we destroy release branch, merging the changes (if any) to master and develop. Feature: when we start a new development, we create a new feature branch and we destroy it when we merge it to Develop. Hotfix: if there is a critical bug in production, we create a temporary hotfix branch and we destroy it when we solve it, merging the changes to master and develop.Advantages There is a clear and well defined way to proceed, which cover most of the situations that we will find during our software life-cycle There is a clear mapping between environments and branches: Develop -&gt; Dev Environment Relese -&gt; Test Environment Master -&gt; Production Environtment It is a well known flow broadly used. Some tools allows you to automatise the flow configuring gitflow branches (for example SourceTree). You are sure that your versions are following a quality process as they progress through the workflow.Disadvantages You need to educate your team to use branches properly. Sometimes to you need to merge into two branches (Releases and Hotfix).Github flowGithub flow is a simplified flow, which only two kind of branches: Master: this branch has infinite lifetime and always there is a change on Master, this should be deployed. Feature/Bugfix branch: This branches are created to add a modification, and they are destroyed after merging to master.Advantages Really simple flow, which works really well for open source projects. CI/CD friendly, as you deliver each change as soon as it comes to master branch.Disadvantages There is not a quality assurance stage for new releases, which can be not realistic for enterprise projects. It doesn’t allow to hotfix a production version without delivering a new version from master. There is not a clear matching between environments and branches, which can lead to misunderstandings. Assuming master is free errors is not realistic and it can be risky.Gitlab flowGitlab flow is a branching strategy which tries to combine the simplicity of Github flow with the environments-oriented branching strategy of Gitflow. As a result, there are several workflows, depending on the needs.Production branch with GitLab flow: it is the result of adding a production branch to Github flow.Advantages You don’t need to deploy each merge that is completed on master, which gives you more control over the delivery. You always have a snapshot of what you have in production in production branch. There is only one way for merges.Disadvantages There is not a ¡release validation stage to make stable in isolation what you are going to deliver to production. There is not a way to hotfix production branch without delivering all the changes you have on master, which can be a big limitation, depending on your delivery pipeline. As for Github flow, you have to assume that master is free of errors, which can be not realistic.Environment branches with GitLab flow: this flow proposes to have an infinite life-time branch per environment.Advantages There is mapping between branches and environments.Disadvantages Same disadvantages as for the previous approach.ConclusionsNow that you know the most relevant git branching strategy, you might know which is the one that fits for your project.Based on my personal experience, simple approaches like Github flow or Gitlab flow are cool for open source projects or when there is not a third party client. However, I think in those approaches there are gaps and more risk involved when it comes to enterprise projects. Even when you have a decent set of automatic testing, for enterprise projects we normally have a QA stage in order to ensure that the new release that you are delivering to your clients is stable.In addition, if you are not in charge of delivering timelines or you have several production clients, the concept of CI/CD per each PR will be problematic as you cannot decide what you want to deliver and when you are going to deliver a new version.Finally, your clients might require a hotfix for the current production version, but they don’t want any new functionality. For this scenario, Gitflow is the only one which can support this by definition." }, { "title": "Benefits of using TypeScript in your project", "url": "/posts/Benefits-of-using-TypeScript-in-your-project/", "categories": "frontend, good, practices, spa, typescript", "tags": "", "date": "2019-07-02 09:10:28 +0200", "snippet": "IntroductionSince I started my career in Computer Sciences, I have had the chance to work with a huge variety of frontend and backend technologies. During that period, on the client side, I witnessed how the web has evolved, starting static html, then adding javascript, AJAX, later animations with flash, silverlight or applets, html5 and recently new frontend frameworks like Angular, Vue and React…..or the incoming and promising Blazor, of which I will talk in a future post.So Javascript at the beginning was created to add some scripting capabilities to our static websites, which most of the times led to a lot of spaghetti code difficult to maintain. Ajax was also an important turning point, which allowed us to render our page dynamically, instead of having that ugly user experience produced by server-side rendering frameworks, such as MVC, JSP or PHP. And this fact moved the logic for web pages from server to client, producing a great client experience. However webs become complex systems developed with a language which was initially designing to develop basic scripts.But then, modern frontend frameworks appeared. In particular, I had the change to migrate a “javascript SPA”, which was really chaotic and difficult to maintain, to Angular (starting with version 2 beta), which included out of the box some first class features, such as modules, Dependency Injection, MVVM pattern, double binding, and my favourite: Typescript.Currently, I’m working in a React project, which started using JavaScript. As a contribution, I added TypeScript support and I migrated existing JS code to TS. In this post, I would like to explain which are the advantages that TypeScript can offer you, in case you didn’t know or you are considering to use typescript in a new or an existing project.What is Typescript?First things first, lets define TypeScript:TypeScript is a programming language developed and maintained by Microsoft. It is a strict syntactical superset of JavaScript, and adds optional static typing to the language.According to the official web page (https://www.typescriptlang.org/), they define typescript as “javascript that scales”, and this feature makes it a really interesting option for the current web paradigm, where we develop complex and complete client side applications (known as Single Page Applications), which need to follow the best software principles in order to have maintainable and scalable applications.TypeScript is designed for development of large applications and transcompiles to JavaScript, which means, your TypeScript code will produce JavaScript, and this JavaScript will be execute by the browser.TypeScript is an open source project, so you can have a look at the source code if you are interested: https://github.com/microsoft/TypeScriptWhy Typescript?Typescript is a really great technology, but you shouldn’t choose a technology because it is cool or trendy. You should choose a technology because of the advantages that it can provide for your team and your project. For this reason, in this section I will try not to be too technical and I will focus on which are the benefits why I chose to use Typescript in my current React project, so the same advantages might apply for you too.Bugs reductionThis is my favourite advantage and that for that reason it is the first in my list. Typescirpt provides code compilation and types checking, which help to detect bugs for typos, names updating, types misusing, etc. In the past, I’ve heard arguments against typescript like “the cool part of javascript is not-typing things” or “typing things will be time consuming”. But having a robust code, will save you a lot of time for bugfixing and maintenance, so I think it is absolutely rewarding. In fact, this can be more important that we think. As a example, Airbnb studied the improvement achieved and they concluded that 38% of bugs could have been avoided by using typescript.Build maintainable code with SOLID principlesThose who are fullstack developers, like me, know very well the advantages of both worlds: frontend and backend. While frontend has been evolving a lot in the latest years, backend development was already robust and based on best practices well known and accepted by the community.With the introduction of typescript, we can design and implement our frontend software with object oriented programming in a similar way as we have been done for many years in backend. The first consequence of this fact is that we can use design patterns, which will help us to build high maintainable software. As an example of this, here you have a repository where you can find all the most relevant design patterns implemented in typescript: https://github.com/totaldesigner/typescript-design-patterns.But that is not all, going further, by using typescript we can also apply SOLID principles to our legacy javascript applications and architectures based on the best practices. This will produce highly maintainable and evolutionary applications. Here we can see an example for node with typescript, where SOLID principles are implemented: https://dev.to/remojansen/implementing-the-onion-architecture-in-nodejs-with-typescript-and-inversifyjs-10ad.Faster DevelopmentIf you are technical, I’m pretty sure you will find this point redundant. However, this is a point I would like to highlight too, since business people sometimes tend to think technology update is not really a necessity for the business. But building highly maintainable and scalable applications will lead to saving a large amount of time, or in other words, budget.In addition, more robust code will involve less bugfixing, so again it will mean saving time and also improving the reputation with your clients. Last, this will keep your developers happier, as it is really frustrating wasting your time fixing bugs that could have been avoided and maintaining an application which is really tedious to maintain or refactor.Flexibility, compatibility and easy migration from JavaScriptOne of the biggest advantages if you are considering to migrate a legacy javascript project to typescript is the fact that typescript is really flexible and compatible with javascript. Typescript is highly configurable and you decide if you want to make it compatible with javascript and not require types.In my current project, this is the approach I followed. First, I configured it to support javascript and undefined types (‘any’ type in typescript language). However, right now, we converted all the javascript files to typescript and we are defining types for models progressively.However, for new developments, you should define types if you added typescript support. This can be challenging for people who are used to developing javascript, but please, don’t be lazy defining types.Clear Api DefinitionType definitions help you when you are consuming an Api, it doesn’t matter if it is an Api library or a web Api. In addition, having types will help you to make your code functional programming oriented, so consumers of your api will easily know what they should provide to call a method and what result they should expect.Having types definition is crucial when you want to share your code with others. In fact, (spoiler alert), I will write an article about this to demonstrate how easy-to-use can be our Api by using typescript.Browser compatibilityTypescript allows you to transpile javascript code which will be compatible with multiple browsers and versions. I know this can be also achieved with ECMA Script and babel, which is great, but in typescript you will have that out of the box. In fact, ECMA script and typescript are closer and closer in functionality, but there are some important features only supported by typescript, such as types, interfaces, generics, etc.ConclusionIn this article we have seen the benefits of using typescirpt in your project. I talked more about React in this article because I recently added TypeScript in a React project, but I strongly recommend it also for other technologies like Vue.js, Node.js, etc, since the benefits are identical.If you are considering typescript in a new project, this is the best time to do it, as you will have your code typed from the beginning. But it is also a really good option for legacy javascript projects, since we can transform it progressively. In fact, this is the scenario where I’m applying it nowadays, and day-by-day we have more typed code. As a consequence, my project is becoming more maintainable and scalable.Finally, I would like to recommend two courses to learn typescript: (Beginner) Getting Started with TypeScript; https://app.pluralsight.com/library/courses/typescript-getting-started/table-of-contents (Beginner) TypeScript Fundamentals: https://app.pluralsight.com/library/courses/typescript/table-of-contents (Advanced) Advanced TypeScript https://app.pluralsight.com/library/courses/typescript-advanced/table-of-contentsIf you don’t have PluralSight subscription don’t worry, remember that the best source of information is always the official documentation: https://www.typescriptlang.org/docs/home.html" }, { "title": "Madrid DotNet 2019 summary", "url": "/posts/Madrid-DotNet-2019-summary/", "categories": "2019, dotnet, dotnet2019, events, microsoft", "tags": "", "date": "2019-06-24 20:14:50 +0200", "snippet": "Thanks to Fenergo, this week I had the chance to attend DotNet 2019 in Madrid and it was a really rewarding experience. For this reason, I would like to post a summary of the day and the key point of the speeches I could attend.First of all, the event started with a keynote by the Scott Hunter. He talked about the innovations incoming to the dotnet world. Some of the key points mentioned were: the introduction of Blazor, ML.net, c# 8.0, .net core 3.0 recent features like performance improvement and the future .NET 5, which will the only one version for .net to support Windows, Linux, macOS, iOS, Android, tvOS, watchOS and WebAssembly, etc.Next, I attended the session “SSR with typescript, React and NextJS” by Carlos Bastos Pérez. In this session, Carlos showed how he does an analysis to choose a technology stack. In the case of NextJS, it allow us to build server side render react applications easily, which can be a really suitable solution when we need to support SEO in our SPA or to support browsers which don’t support javascript. In addition, this can also provide performance improvement depending on the scenario. Regarding typecript, Carlos mentioned how it can be the key to produce code more maintainable and to reduce the amount of bugs in our code. Carlos also recommended the package typestyle, which allow us to type our css, so we don’t end up with mess of classes repeated in our code. Here we have the example of the demo for this session: https://github.com/cbastos/dotnet-2019-ssr-typescript-react-nextjsIn the following session, “Asp.Net Core Good practices”, Carlos Landeras and Luis Ruiz Pavón talked about different techniques to improve our dotnet applicaitons, such as code instrumentation, resilience, health checks, performance improvements with HttpContext Pipes, Endpoint routing, etc. In addition, they have shared a repository with examples of all these good practices here: https://github.com/CarlosLanderas/dotnet2019-aspnet-core-best-practicesIn next session, “Introduction to Blazor”, Ryan Nowak showed the current state of Blazor, the new dotnet framework based on Web Assembly, which will allow us to develop single page applications in C#. During the presentation Ryan showed examples of client and sever rendering with Blazor.In the next presentation, “Improving Vue with typescript, inversify and vuex”, Quique Fdez Guerra talked about the benefits of using technologies like typescript or inversify in our frontend application, in this case Vue. Quique highlighted how, apart from the technology used, typescirpt and inversify can help you to produce maintainable code based on dependency inversion and other patterns and good practices.Last but not least, Hugo Biarge presented “Authentication in native and web applications”, where he talked about different authentication scenarios, like: Local website, Applications using active directory and Applications using a OIDP as a Federation Gateway. The repository with the presented examples is here: https://github.com/hbiarge/authentication-samplesIn conclusion, Dotnet 2019 was a really interesting conference, where I had the chance to learn new things and meet awesome people. I’m really looking forward to Dotnet 2020, which will be an event event greater, as Pablo Pérez (the CEO of Plain Concepts) announced.I’m also preparing some posts related to the technologies presented, as there are some technologies I would like to investigate further. So, if you are interested in web development, stay tuned.Happy coding!" }, { "title": "Azure Functions, the future of SaaS", "url": "/posts/Azure-Functions,-the-future-of-SaaS/", "categories": "azure, functions, DI, OpenApi, SaaS, Visual, Studio", "tags": "", "date": "2019-03-19 10:39:34 +0100", "snippet": "IntroductionRecently, I have had the chance to use Azure Functions as candidate for the design and development of a service, which will be delivered as Software as a Service (SaaS). Azure Functions is a relatively recent technology, and as a consequence, I have realized many people didn’t heard of it yet. For this reason, I would like to write this post as a brief introduction and my personal experience using it at the present.What is Azure Functions?Azure Functions provide a framework to build and manage easily serverless oriented architectures. The idea behind Azure Functions  is that you have a piece of code, which provides a functionality. In order to execute your piece of code, Azure functions introduce the concept of “trigger”. A trigger can be an HTTP request, a timer, a message in a queue, a modification on a Blog, etc. In addition, Azure Functions provides the concept of “bindings” Binding to a function is a way of declaratively connecting another resource to the function; bindings may be connected as input bindings, output bindings, or both. Data from bindings is provided to the function as parameters. Here you can see the triggers&amp;bindings supported by Azure functions at this moment: https://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindingsAzure Functions offers build-in runtime scaling and you will pay only for the request you have and the resources you need in your functions. This business model makes specially interesting Azure Functions when you want to deploy your business logic without caring about the infrastructure. (More info here: https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale)Currently, Azure Functions implements two versions. Version 1.x is supports .NET development from the portal or Windows machines and uses the .NET Framework. 1.x supports C#, JavaScript, and F#, with experimental support for Python, PHP, TypeScript, Batch, Bash, and PowerShell.Version 2.x is in preview. It leverages .NET Core and supports cross-platform development on Windows, macOS, and Linux machines. 2.x adds first-class support for Java but doesn’t yet directly support any of the experimental languages. Version 2.x uses a new binding extensibility model that enables third-party extensions to the platform, independent versioning of bindings, and a more streamlined execution environment.In my last approach, I have used version 2, as I wanted to get all the benefits introduced by dotnet core.Creating a new Azure Function from AzureOne of the key features of Azure Function is the flexibility to turn your code into functionality. For this reason, you can write your function directly on Azure and it will be available without care about deployment, infrastructure, etc.To do so, go to your azure account and create a new Azure App. Then, choose In-portal.You will be able to select several triggers. For this example, we will use Webhook + Api:Now, the only thing left to do is write your code. For this example, I will implement sum function, so the sum of two parameters will be returned as a result.From the same Azure screen, you can test your function and we see the result obtained is as we expected.Creating a new Azure Function from Visual StudioCreating functions from Azure portal can be handy, but it might not be the best choice to develop complex functions, where you want to use a code repository, automatic testing, CI/CD pipelines, etc. For real scenarios, you should use Visual Studio to develop your Azure Functions. However, it is as easier as creating them on Azure Portal.To create a new Azure Function in Visual Studio, first select a new Azure Functions project:Then, you will be able to select version, triggers, storage and access rights. For this example I will use Http trigger with version 2 (.net core)Now all you have to do is writing your code, since the first time you execute it, Azure functions Tools will be installed automatically.  If you execute your function on a browser you will see something like this:Adding OpenApi definition and Dependency InjectionOnce you start developing your Azure Functions as Azure Http triggers, you might want to add OpenApi definition for your clients and Dependency Injection to keep decoupled the implementation of your functions. And here is when you will realize Azure Functions might be not as mature as asp.net core, as it doesn’t include this functionality out-of-the-box.Fortunately, you will be able to find some temporary solutions until Azure Functions version 2 include it.For example, for OpenApi, there is a NuGet package available, which allows you to use OpenApi with Swagger UI. https://devkimchi.com/2019/02/02/introducing-swagger-ui-on-azure-functions and the same for DI https://devkimchi.com/2019/02/22/performing-constructor-injections-on-azure-functions-v2.Here you can see all the source code to use those NuGet packages with Azure Functions on my GitHub repo: https://github.com/albertocorrales/sum-azure-functionAfter configuring OpenApi and DI NuGet packages, you can see the example of this post running with Swagger going to the url http://localhost:7071/api/swagger/uiHowever, from my point of view, this is a workaround and there are some disadvantages if we compare with dotnet core: To use Swagger plugin, you need to have a lot of classes (maybe these clases shoudl be included within the NuGet package) Swagger doesn’t generate automatic documentation from XML comments for WebApi To support DI in your functions, you have to implement an empty interface (bad practice), which implements IFunction. However, there is another approach, which simplifies this and support [Inject] attribute in your http trigger definition https://github.com/BorisWilhelms/azure-function-dependency-injection To load DI in StartUp class, your application must target .net standard 2.0. If you try to target .net core it doesn’t work. ConclusionsAzure Functions is a really promising technology and it might the future of SaaS and WebApi development. This is reason why all the competitors have their own implementation of the same concept. Some of them are: Azure functions, AWS lambdas, IBM OpenWhisk, Iron.io, Google Cloud functions, etc.In spite of the fact that Azure Functions might seem an Azure technology, it supports Docker, so you can containerize your azure functions and deliver them as a container to other cloud technology or even to on-premise environments. The possibilities of Azure Functions are endless.However, as we analyzed in this article, Azure Functions version 2 is still in preview version and there are some missing features, such as OpenApi or DI. In spite of the fact that there are some NuGet packages to provide this functionality, those are workarounds and the implementation has some lacks, if we compare with dotnet core.For this reasons, until Azure Functions includes the missing functionality, I would suggest to use asp.net core to develop your Web Apis in the meantime. But keep an eye on Azure Functions, as it is very promising technology and it will be soon the future of SaaS.Learn more about Azure FunctionsIf you want to learn more about Azure Functions, I recommend you the following material: (eBook) Serverless apps: Architecture, patterns, and Azure implementation https://dotnet.microsoft.com/learn/cloud/azure-architecture Official Documentation: https://docs.microsoft.com/en-us/azure/azure-functionsPluralsight courses: Microsoft Azure Developer: Create Serverless Functions https://app.pluralsight.com/library/courses/microsoft-azure-serverless-functions-create/table-of-contents Writing and Testing Precompiled Azure Functions in Visual Studio 2017 https://app.pluralsight.com/library/courses/azure-functions-visual-studio/table-of-contentsAzure functions is open source, so you can have a look at the source code: Azure Functions project: https://github.com/Azure/Azure-Functions" }, { "title": "Migrating legacy projects to .Net Core/Standard", "url": "/posts/Migrating-legacy-projects-to-.Net-CoreStandard/", "categories": ".net, core, standard, legacy, migration., tools, useful", "tags": "", "date": "2019-01-07 09:42:52 +0100", "snippet": "IntroductionAfter the second version of .NET Core, it is pretty likely to be the future for .NET development, at least for many years. Luckily for legacy projects, Microsoft still continues supporting .NET Framework and developing new versions, such as .NET framework 4.8, which currently available as early access https://github.com/Microsoft/dotnet-framework-early-access. Even though, .NET Core provides some benefits that might be considered to port your legacy projects from .NET Framework to .NET Core. According to this article Choosing between .NET Core and .NET Framework for server apps, these are the points that you should consider to choose between .NET Core or .NET Framework:Use .NET Core for your server application when: You have cross-platform needs. You are targeting microservices. You are using Docker containers. You need high-performance and scalable systems. You need side-by-side .NET versions per application.Use .NET Framework for your server application when: Your app currently uses .NET Framework (recommendation is to extend instead of migrating). Your app uses third-party .NET libraries or NuGet packages not available for .NET Core. Your app uses .NET technologies that aren’t available for .NET Core. Your app uses a platform that doesn’t support .NET Core.So as conclusion, use .NET Core if you can, because there are more benefits. For this reason, if you start a new project you should consider .NET Core as first option. But, what happens with legacy projects? According to the previous article, they recommend extend legacy project with .NET Core, instead of migrating. However, I would say “it depends on the project”. Calculate the cost of migrating and then decide, because using .NET Core will be rewarding in the long run if it is possible to migrate your project easily.In addition, if you have some cross-cutting infrastructure libraries shared by your different services, you should consider a migration to .NET Standard for those libraries, so new micro-services or projects will be able to use .NET Core. Remember you can make your libraries compatible with .NET Framework and .NET Standard at the same time, which can be very handy to support legacy and new projects.Migrating analysis for Legacy Projects to .NET Core/.NET StandardTo illustrate how we can analyze the migration of a legacy project to .NET Core/Standard, I will use a DDD example project shared as open source on GitHub: https://github.com/sandipray63in/DDD-Based-Enterprise-Application-FrameworkIn order to analyze the migration of a legacy project, we can use a very useful Visual Studio extension called .NET Portability Analyzer. You can download this extension here: https://marketplace.visualstudio.com/items?itemName=ConnieYau.NETPortabilityAnalyzer.After installing .NET Portability Analyzer, we can configure which parameters we want to analyze in or project. To do so, right click on your solution and you will see a new option “Portability Analyzer Settings”.In Options window, you can set up your settings, depending on the analysis you want to generate.Now, in right-click menu, select “Analyze Assembly Portability” and after a few seconds (depending on the size of your project), you will have a new report. For the DDD project example, we can see we have a very high rate of compatibility with .NET Core. The project more problematic would be RestfulWebAPI with a 74.95% of compatible code.In the second tab of the report, we can see the details of the compatibility problems that we have, so we can see the assemblies with classes and namespaces not supported by each platform.Considering this report, we should have an estimation of the different incompatibilities of our project to migrate it to .NET Core/Standard. Depending of the incompatibilities, we should see if we have a similar functionality in .Net Core/Standard or we have to change our source code, which would involve extra work.I hope you found this post useful.Happy conding! :)" }, { "title": "Script to clean your Visual Studio solution folder", "url": "/posts/Script-to-clean-your-Visual-Studio-solution-folder/", "categories": "clean, projects, cmd, script, tools, useful", "tags": "", "date": "2018-12-09 12:24:14 +0100", "snippet": "Sometimes, cleaning your solution folder comes in handy when you want to build from scratch. If you don’t do so, you can have unexpected results because of the files generated by your previous builds.To carried out the cleaning task, I use a simple script which allows me to configure a list of folders I want to remove. This script is divided in two parts. The first part is just a script to check if we have admin permissions. If not, it will display a pop-up to get admin permissions.You can skip this part, but it is very handy, since the cleaning part might need admin permissions. This part can be very useful for all your scripts, which need admin permissions.:: BatchGotAdmin:-------------------------------------REM  --&gt; Check for permissions    IF \"%PROCESSOR_ARCHITECTURE%\" EQU \"amd64\" (&gt;nul 2&gt;&amp;1 \"%SYSTEMROOT%\\SysWOW64\\cacls.exe\" \"%SYSTEMROOT%\\SysWOW64\\config\\system\") ELSE (&gt;nul 2&gt;&amp;1 \"%SYSTEMROOT%\\system32\\cacls.exe\" \"%SYSTEMROOT%\\system32\\config\\system\")REM --&gt; If error flag set, we do not have admin.if '%errorlevel%' NEQ '0' (    echo Requesting administrative privileges...    goto UACPrompt) else ( goto gotAdmin ):UACPrompt    echo Set UAC = CreateObject^(\"Shell.Application\"^) &gt; \"%temp%\\getadmin.vbs\"    set params= %*    echo UAC.ShellExecute \"cmd.exe\", \"/c \"\"%~s0\"\" %params:\"=\"\"%\", \"\", \"runas\", 1 &gt;&gt; \"%temp%\\getadmin.vbs\"    \"%temp%\\getadmin.vbs\"    del \"%temp%\\getadmin.vbs\"    exit /B:gotAdmin    pushd \"%CD%\"    CD /D \"%~dp0\":--------------------------------------The second part of the script, is the cleaning part, where the script will remove recursively all the folders you configure, from the current path to all the subfolders. In my case, I usually remove BIN, OBJ and PACKAGES folders, so this is my script::: Remove folders bin, obj and packages@echo Deleting all BIN, OBJ and PACKAGES folders…for /d /r . %%d in (bin,obj,packages) do @if exist \"%%d\" rd /s/q \"%%d\"@echo BIN and OBJ folders successfully deleted. Close the window.pause &gt; nulYou can find the .bat on my GitHub: https://github.com/albertocorrales/script-clean-sln/blob/master/clean_solution.batI hope you found this post useful,Happy coding :)" }, { "title": "Why you should migrate your old csproj to SDK format", "url": "/posts/Why-you-should-migrate-your-old-csproj-to-SDK-format/", "categories": "2017, csproj, NuGet, tips, &, tricks, Visual, Studio", "tags": "", "date": "2018-12-02 22:27:03 +0100", "snippet": "Introduction With the introduction of Visual Studio 2017, the format used for csproj was updated including several advantages. This new format will be used when you create a new .Net Core project or .Net Standard library. In spite of the fact that VS2017 continues supporting old csproj format, you can migrate your old projects to SDK format. As a result, you will have the following benefits: Cleaner csproj files. With new format, a lot of things can be omitted, so your csproj files will be cleaner. Files are included using a wildcard, instead of including every single file. Solution project references are simplier. NuGet reference packages as PackageReferences, so you won’t need packages.config and you will be able to use wildcard in your references (for example, 1.0.*). NuGet package definition as a part of the project file. You won’t need more nuspec files. Definition of AssemblyInfo attributes in csproj, you won’t need more AssemblyInfo.cs files. In addition, you will be able to use features such as Directory.Build.props to centralize this info. Go to this link for more info: https://www.albertocorrales.com/2018/11/maintaining-assemblyinfo-for-multiple.html Your project will be ready to use .Net Standard 2 as a target if you want to do it in the future.Migrate your csproj to SDK format1) Migrating from Visual Studio (only PackageReferences)From Visual Studio, there is not an automatic way to convert your projects to SDK format. The only thing VS allow you to do form IDE is converting NuGet references to PackageReference format. To do it, just select “references” or “packages.config” and click on “Migration packages.config to PackageReferences”.In order to migrate ASP.NET projects, you won’t see the previous menu, so you will have to do the following workaround: Open the csproj Replace the ProjectGuid by {7C796B6B-86B5-4C57-ADAA-12CF1FECDA71} and remove ProjectTypeGuids Now right click on reference, you should be able to migrate Put back the previous ProjectType and ProjectTypeGuids2) SDK format manual migrationIn order to convert your csproj, the default method is manual conversion. To do it, the easiest way is starting your csproj from the scratch. To know all the things you can remove, you can follow this tutorial, which explains the conversion process: https://natemcmaster.com/blog/2017/03/09/vs2015-to-vs2017-upgrade/3) SDK format migration using Tool CsprojToVs2017Manual mode can be useful if you have few project, but it is difficult to scale if you have a lot of projects to convert. Luckily, there is a GitHub project named CsprojToVs2017, which can be very handy to convert multiple projects automaticly: https://github.com/hvanbakel/CsprojToVs2017This tool is very helpful, but before migration, you will see the resulting conversion it not as clearer as it should, you can finish the migration manually by removing unnecessary parts in csproj.Migration ExampleOld csprojHere we have an example of a migration for a project. This is the old version:&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;Project ToolsVersion=\"15.0\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\"&gt; &lt;Import Project=\"$(MSBuildExtensionsPath)\\$(MSBuildToolsVersion)\\Microsoft.Common.props\" Condition=\"Exists('$(MSBuildExtensionsPath)\\$(MSBuildToolsVersion)\\Microsoft.Common.props')\" /&gt; &lt;PropertyGroup&gt; &lt;Configuration Condition=\" '$(Configuration)' == '' \"&gt;Debug&lt;/Configuration&gt; &lt;Platform Condition=\" '$(Platform)' == '' \"&gt;AnyCPU&lt;/Platform&gt; &lt;ProjectGuid&gt;{214F3A00-13FC-43BF-AFEF-A740C8B5461E}&lt;/ProjectGuid&gt; &lt;OutputType&gt;Library&lt;/OutputType&gt; &lt;AppDesignerFolder&gt;Properties&lt;/AppDesignerFolder&gt; &lt;RootNamespace&gt;LibraryExample&lt;/RootNamespace&gt; &lt;AssemblyName&gt;LibraryExample&lt;/AssemblyName&gt; &lt;TargetFrameworkVersion&gt;v4.7&lt;/TargetFrameworkVersion&gt; &lt;FileAlignment&gt;512&lt;/FileAlignment&gt; &lt;Deterministic&gt;true&lt;/Deterministic&gt; &lt;/PropertyGroup&gt; &lt;PropertyGroup Condition=\" '$(Configuration)|$(Platform)' == 'Debug|AnyCPU' \"&gt; &lt;DebugSymbols&gt;true&lt;/DebugSymbols&gt; &lt;DebugType&gt;full&lt;/DebugType&gt; &lt;Optimize&gt;false&lt;/Optimize&gt; &lt;OutputPath&gt;bin\\Debug\\&lt;/OutputPath&gt; &lt;DefineConstants&gt;DEBUG;TRACE&lt;/DefineConstants&gt; &lt;ErrorReport&gt;prompt&lt;/ErrorReport&gt; &lt;WarningLevel&gt;4&lt;/WarningLevel&gt; &lt;/PropertyGroup&gt; &lt;PropertyGroup Condition=\" '$(Configuration)|$(Platform)' == 'Release|AnyCPU' \"&gt; &lt;DebugType&gt;pdbonly&lt;/DebugType&gt; &lt;Optimize&gt;true&lt;/Optimize&gt; &lt;OutputPath&gt;bin\\Release\\&lt;/OutputPath&gt; &lt;DefineConstants&gt;TRACE&lt;/DefineConstants&gt; &lt;ErrorReport&gt;prompt&lt;/ErrorReport&gt; &lt;WarningLevel&gt;4&lt;/WarningLevel&gt; &lt;/PropertyGroup&gt; &lt;ItemGroup&gt; &lt;Reference Include=\"CommonServiceLocator, Version=2.0.4.0, Culture=neutral, PublicKeyToken=489b6accfaf20ef0, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\CommonServiceLocator.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"Newtonsoft.Json, Version=11.0.0.0, Culture=neutral, PublicKeyToken=30ad4fe6b2a6aeed, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Newtonsoft.Json.11.0.2\\lib\\net45\\Newtonsoft.Json.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"System\" /&gt; &lt;Reference Include=\"System.Configuration\" /&gt; &lt;Reference Include=\"System.Configuration.ConfigurationManager, Version=4.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\System.Configuration.ConfigurationManager.4.4.1\\lib\\net461\\System.Configuration.ConfigurationManager.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"System.Core\" /&gt; &lt;Reference Include=\"System.Xml.Linq\" /&gt; &lt;Reference Include=\"System.Data.DataSetExtensions\" /&gt; &lt;Reference Include=\"Microsoft.CSharp\" /&gt; &lt;Reference Include=\"System.Data\" /&gt; &lt;Reference Include=\"System.Net.Http\" /&gt; &lt;Reference Include=\"System.Xml\" /&gt; &lt;Reference Include=\"Unity.Abstractions, Version=3.3.1.0, Culture=neutral, PublicKeyToken=6d32ff45e0ccc69f, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\Unity.Abstractions.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"Unity.Configuration, Version=5.2.5.0, Culture=neutral, PublicKeyToken=6d32ff45e0ccc69f, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\Unity.Configuration.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"Unity.Container, Version=5.8.11.0, Culture=neutral, PublicKeyToken=489b6accfaf20ef0, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\Unity.Container.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"Unity.Interception, Version=5.5.5.0, Culture=neutral, PublicKeyToken=489b6accfaf20ef0, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\Unity.Interception.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"Unity.Interception.Configuration, Version=5.1.7.0, Culture=neutral, PublicKeyToken=489b6accfaf20ef0, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\Unity.Interception.Configuration.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"Unity.RegistrationByConvention, Version=2.1.9.0, Culture=neutral, PublicKeyToken=489b6accfaf20ef0, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\Unity.RegistrationByConvention.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;Reference Include=\"Unity.ServiceLocation, Version=2.1.2.0, Culture=neutral, PublicKeyToken=489b6accfaf20ef0, processorArchitecture=MSIL\"&gt; &lt;HintPath&gt;..\\packages\\Unity.5.8.11\\lib\\net47\\Unity.ServiceLocation.dll&lt;/HintPath&gt; &lt;/Reference&gt; &lt;/ItemGroup&gt; &lt;ItemGroup&gt; &lt;Compile Include=\"Class1.cs\" /&gt; &lt;Compile Include=\"Properties\\AssemblyInfo.cs\" /&gt; &lt;/ItemGroup&gt; &lt;ItemGroup&gt; &lt;ProjectReference Include=\"..\\LibraryExample2\\LibraryExample2.csproj\"&gt; &lt;Project&gt;{a984ea74-aa5d-44ca-a404-2ed068417af9}&lt;/Project&gt; &lt;Name&gt;LibraryExample2&lt;/Name&gt; &lt;/ProjectReference&gt; &lt;/ItemGroup&gt; &lt;ItemGroup&gt; &lt;None Include=\"packages.config\" /&gt; &lt;/ItemGroup&gt; &lt;Import Project=\"$(MSBuildToolsPath)\\Microsoft.CSharp.targets\" /&gt;&lt;/Project&gt;New csprojTo migrate this project I used CsprojToVs2017 tool and then I removed unnecessary parts manually. Notice migrated csproj included nuspec and AssemblyInfo information, so those files were removed.&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt; &lt;PropertyGroup&gt; &lt;ProjectGuid&gt;{214F3A00-13FC-43BF-AFEF-A740C8B5461E}&lt;/ProjectGuid&gt; &lt;TargetFramework&gt;net47&lt;/TargetFramework&gt; &lt;AssemblyTitle&gt;LibraryExample&lt;/AssemblyTitle&gt; &lt;Product&gt;LibraryExample&lt;/Product&gt; &lt;Copyright&gt;Copyright © 2018&lt;/Copyright&gt; &lt;/PropertyGroup&gt; &lt;ItemGroup&gt; &lt;PackageReference Include=\"Newtonsoft.Json\" Version=\"11.0.2\" /&gt; &lt;PackageReference Include=\"System.Configuration.ConfigurationManager\" Version=\"4.4.1\" /&gt; &lt;PackageReference Include=\"System.Reflection.Emit\" Version=\"4.3.0\" /&gt; &lt;PackageReference Include=\"Unity\" Version=\"5.8.11\" /&gt; &lt;/ItemGroup&gt; &lt;ItemGroup&gt; &lt;Reference Include=\"System.Configuration\" /&gt; &lt;Reference Include=\"System.Data.DataSetExtensions\" /&gt; &lt;Reference Include=\"Microsoft.CSharp\" /&gt; &lt;Reference Include=\"System.Net.Http\" /&gt; &lt;/ItemGroup&gt; &lt;ItemGroup&gt; &lt;ProjectReference Include=\"..\\LibraryExample2\\LibraryExample2.csproj\" /&gt; &lt;/ItemGroup&gt;&lt;/Project&gt;One think I like is that after migration, you will see the references in Visual studio organized into 3 categories: Assemblies, NuGet and Projects. As a consequence. you don’t need to find out the source of a reference exploring the csproj content.I hope you found this article useful,Happy coding!!" }, { "title": "Maintaining AssemblyInfo for multiple projects", "url": "/posts/Maintaining-AssemblyInfo-for-multiple-projects/", "categories": "Custom, Build, MSBuild, NuGet", "tags": "", "date": "2018-11-27 00:10:44 +0100", "snippet": " When you are developing an application or manly a NuGet package, you might want to keep the same AssemblyInfo for all your packages, which involves updating each project when you want to publish a new version of your package according to the SemVer convention [Semantic Versioning 2.0.0 Semantic Versioning](https://semver.org/). In order to make it easier, with the improvements of dotnet core and the new csproj syntax, which I strongly recommend, MSBuild 15 introduced a pretty cool feature: Solution-wide project properties with Directory.Build.props [Customize your build - Visual Studio Microsoft Docs](https://docs.microsoft.com/en-us/visualstudio/msbuild/customize-your-build?view=vs-2017). Basically, this allows you to define certain properties and use them in all your project in a centralised way, so you don’t have to update your projects one by one. All you have to do is create a new text file named Directory.Build.props and place it where you have your solution file. Here is an example of the properties you can use:‍&lt;Project&gt;  &lt;PropertyGroup&gt;    &lt;Version&gt;1.1.0&lt;/Version&gt;    &lt;FileVersion&gt;1.1.0&lt;/FileVersion&gt;    &lt;Product&gt;Fenergo.Platform.Common&lt;/Product&gt;    &lt;Company&gt;Fenergo&lt;/Company&gt;    &lt;Copyright&gt;Copyright © Fenergo 2018&lt;/Copyright&gt;  &lt;/PropertyGroup&gt;&lt;/Project&gt;‍‍‍‍‍‍‍‍‍In addition, you can inherit another funcionalities such as analysers. For example, if you want to use the Stylecop.Analyzers for all your project, you can add a Directory.Build.props like this:&lt;Project&gt;  &lt;PropertyGroup&gt;    &lt;TreatWarningsAsErrors&gt;True&lt;/TreatWarningsAsErrors&gt;  &lt;/PropertyGroup&gt;  &lt;ItemGroup&gt;    &lt;AdditionalFiles Include=\"..\\\\StyleCop.json\" /&gt;  &lt;/ItemGroup&gt;  &lt;ItemGroup&gt;    &lt;PackageReference Include=\"StyleCop.Analyzers\" Version=\"1.0.2\" /&gt;  &lt;/ItemGroup&gt;&lt;/Project&gt;If you don’t have your Directory.Build.props in your root directory, you can import it in the projects which you need it.&lt;Project&gt;  &lt;!-- Import parent Directory.build.props --&gt;  &lt;Import Project=\"../Directory.Build.props\" /&gt;  &lt;!-- Properties common to all test projects --&gt;  &lt;!-- ... --&gt;&lt;/Project&gt;I hope you found this post helpful,Happy coding!Alberto." }, { "title": "Use Visual Studio extension to distribute your Code Snippets", "url": "/posts/Use-Visual-Studio-extension-to-distribute-your-Code-Snippets/", "categories": "extensions, testing, unit, tests, Visual, Studio, VSIX", "tags": "", "date": "2018-11-18 23:55:49 +0100", "snippet": "IntroductionIn a previous post we saw how you can build code snippets to write tests faster empowering your team to follow good practices. https://www.albertocorrales.com/2018/10/writing-unit-tests-faster.htmlIn this post, we will see how we can distribute our snippets with a Visual Studio extension, which offers different advantages, for example: You encapsulate your snippets in a VS extension, you you don’t need to distribute them independently, and developers don’t need to add them to VS one by one. The extension can be updated improving he current snippets or including new snippets. You can add more functionality in your Visual Studio extension.Creating a new Visual Studio extensionFirst of all, if you have never developed a Visual Studio extension, you might need to install Visual Studio extension development kit. To do it, install this option in your Visual Studio InstallerCreate a new Visual Studio extensions project. For this example, I will name it “SnippetsExtension”:In the root of the project, create a new folder named “snippets”. Here you can store all the snippets you wan to distribute with your Visual Studio Extension. For this example, we will used the unittest.snippet example that we saw in the previous post. You can download it here: https://github.com/albertocorrales/unittestsnippet/blob/master/unittest.snippet.For file unittest.snippet, it is important to select the following properties: Build Action: Content Copy to Output Directory: Copy always Include in VSIXIn folder snippets, we will create a new text file and we will name it snippets.pkgdef. This file is used to add certain keys to the registry. As for this example, we are using CSharp, write the following:-—————————————————————-// Csharp[$RootKey$\\Languages\\CodeExpansions\\Csharp\\Paths]“snippets”=”$PackageFolder$”-—————————————————————-For file snippets.pkgdef select the following properties: Build Action: Content Copy to Output Directory: Copy always Include in VSIXIf you followed the previos steps, you should see something like this:Now add your package definition in your VSIX manifest. To do it, double click on source.extensionvsxmanifest. Go to tab “Assets”, and then, select the following options to distribute your snippets with your extension:Finally, test your extension. If you press F5, an experimental instance of Visual Studio will raise, the changes you make in this instance will not affect your development environment. To test your extension is working fine, create a new test project and type “utest” to see the new snippet you added.You can download this example from my GitHub repository. https://github.com/albertocorrales/SnippetsExtensionIf you want more information about how to develop your own Visual Studio extensions, you can go here: https://docs.microsoft.com/en-us/visualstudio/extensibility/starting-to-develop-visual-studio-extensions?view=vs-2017I hope you found this post useful,Happy coding!" }, { "title": "Run Visual Studio as Administrator", "url": "/posts/Run-Visual-Studio-as-Administrator/", "categories": "dev, tip, tips, &, tricks, Visual, Studio", "tags": "", "date": "2018-11-09 14:53:52 +0100", "snippet": "Probably you already knew this trick to run apps in Windows 10 as Administrator. If not, I hope this tip can help you.Sometimes, you need to execute Visual Studio as Administrator always, for example if you are deploying and debugging your application in your Local IIS. To execute VS as Admin there are several ways.1) Using Sub-menus:If you go to the item in Windows menu and click on right, you will see menu “More”, where you can run VS as administrator.2) Using shortcuts:Previous method is slow, so If you don’t want to do this each time you open VS, you can create a shortcut in your Desktop and execute it as Administrator. To do this, press right click on the shortcut, click on Advanced and select Run as administrator.3) Always as Administrator:Method 2 is much better than 1, but you always have to open VS by using your modified shortcut. So for example, you can not open directly a file .sln with Admin rights. But don’t worry, there is a method to execute VS always as Administrator.In the properties of the previous shortcut, go to “Open File Location”. Over devenv.exe press right click and go to Troubleshoot compatibility.Then choose Troubleshoot program and select “The program requires additional permissions”.Finally, save these settings for this program and you won’t have to care anymore about open VS with sub menus or shortcuts to run it as Administrator. You can use the same method for PowerShell or whatever program you want.I hope you find this tip useful.Happy coding." }, { "title": "Writing unit tests faster", "url": "/posts/Writing-unit-tests-faster/", "categories": "best, practices, snippet, testing, unit, tests, Visual, Studio", "tags": "", "date": "2018-11-01 11:04:20 +0100", "snippet": "IntroductionWhen you are writing tests, the name of the test is very important because the person who is executing the test should be able to detect what was fine or wrong at a glance. For this reason, it is a good practice to follow the convention format: TestedMethodName_InputData_ExpectedResultBy using this format, we can see in a line the method to test, the input used and the result expected, so it will be easy to identify what was wrong.Another good practice is following the “Arrange”, “Act”, “Assert” convention (http://wiki.c2.com/?ArrangeActAssert), so when we are reading a test, we can identify easily each part of the test. To make more visible each part, unit test usually include one comment per each state of the test.Finally, if you are using different test categories, for example to classify unit tests and integration tests, you should use the word TestCategory(“YourCategory”) in your metadata definition.Using the snippetIn order to write faster unit tests with this format, I created this snippet which allow you to write the skeleton of a test with by using a shortcut.First we can see the results. When we write “utest” we want our snippet to show up with a description.If we type double TAB, the snippet completes the skeleton of a unit test with the skeleton, the test name with the three parts of the test and the body of the test with the three steps of the test. In addition, when you are writing the name of the test, you can tabulate through the 3 parts. When you press intro, the cursor will go to the after the “Arrange” step.Writing the snippetCode snippets are XML language. I we analyse the snippet for unit tests, we can see two parts: the Header and the Snippet. You can download the complete snippet from my GitHub: https://github.com/albertocorrales/unittestsnippet/blob/master/unittest.snippetIn the Header part, we fill some information about the snippet. The most important part is the Shortcut. In this example I chose “utest” to make the snippet show up. If you are using Resharper, you might either need to switch back to Visual Studio Intellisense in Resharper’s options or use Ctrl+K, X to bring up the snippet menu.On the other hand, in Snippet section, we defined the three parts for the name of our test as “testmethodname”, “inputdata” and “expectedresult”. We can add a tool tip for each part. Finally, we write the code part, where we compose the code that we want to show in Visual Studio. We use the three variables defined in Declarations and the variable $end$ to go there when we press enter.To use your snippet you only have to save this snippet with extension *.snippet in Visual Studio folder: %USERPROFILE%\\Documents\\Visual Studio 2017\\Code Snippets\\Visual C#\\My Code Snippets\\Now you can use your snippet in Visual Studio (reset Visual Studio is not required).If you want more information about snippet you can go here: https://msdn.microsoft.com/en-us/library/ms165394.aspx.The original idea of this snippet comes from this Pluralsight course, which I strongly recommend: Writing Highly Maintainable Unit Tests" } ]
